{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessor as p \n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replacement(text, token_type = \"url\"):\n",
    "    \"\"\"\n",
    "    Function that will take in a block of text and replace the url with a token of some type\n",
    "    text (str): A block of text that contains a url\n",
    "    token_type (str): A specfication on what token should replace the url. Default is \"url\", which just returns \"url\". Other options include: \n",
    "        \"domain\", which returns the domain (e.g., \".gov url\")\n",
    "        \"host\", which returns the host of the website and domain (e.g., \"cdc.gov url\")\n",
    "    \"\"\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # Get a list of all urls\n",
    "    \n",
    "    if urls == []: # If list is blank\n",
    "        return text #Return previous text since nothing needs to change\n",
    "    \n",
    "    new_text = text[:] #Deep copy of the text to a new object\n",
    "    \n",
    "    if token_type == \"url\": #For the base case\n",
    "        for url in urls:\n",
    "            new_text = new_text.replace(url, \"url\") #Replace each url with the \"url\" token\n",
    "\n",
    "    elif token_type == \"domain\": #For the case of just extracting the domain\n",
    "        for url in urls:\n",
    "            try: \n",
    "                urlparse(url).netloc.split(\".\")[-1] #Extract just the domain \n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc.split(\".\")[-1] #Extract just the domain\n",
    "                \n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "            \n",
    "    else: #Extracting full host name\n",
    "        for url in urls:\n",
    "            try:\n",
    "                urlparse(url).netloc #Extract the full host name\n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc #Extract the full host name\n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_score(sid, text, dataframe):\n",
    "    \"\"\"\n",
    "    Function that will take in a text and return an estimated valence. \n",
    "    Note that this assumes that the column names match the VADER output\n",
    "    (i.e., \"neg\", \"neu\", \"pos\", \"compound\")\n",
    "    param sid (str): Name of the SentimentIntensityAnalyzer() defined outside the function \n",
    "    param text (str): A string of text to analyze\n",
    "    param dataframe (DataFrame): The pandas dataframe to append results to\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return(dataframe.append(scores, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (1,2,3,4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/catherinepollack/Documents/dartmouth/research/aim3_facebook_covid19_obesity/data\"\n",
    "file_name = \"/211124_tokenized_sentences_expanded_health_liwc.csv\"\n",
    "health_liwc_labeled = pd.read_csv(str(file_path + file_name))\n",
    "\n",
    "file_name = \"/211124_tokenized_sentences_expanded_nonhealth_liwc.csv\"\n",
    "nonhealth_liwc_labeled = pd.read_csv(str(file_path + file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing text!\n",
      "URL parse error with https://[n.neurology.org/content/63/12/2240.short](http://n.neurology.org/content/63/12/2240.short)\n",
      "URL parse error with http://www.quranrevolution.com](http://www.quranrevolution.com/?fbclid=IwAR1fuqGkhOetgeMuYF9jCT63pFjrPMxPgtuni1I9D7EHR3fn52YbZxIeJw0)\n",
      "URL parse error with https://www.scimedregister.com](https://www.scimedregister.com/)**\n",
      "URL parse error with https://811.novascotia.ca](https://811.novascotia.ca/)\n",
      "URL parse error with https://henote.com](https://henote.com/)/\n",
      "Now processing text!\n",
      "URL parse error with http://www.keenanmckenzie.com]\n",
      "URL parse error with http://abesun.com](http://abesun.com/)\n",
      "URL parse error with https://pelicanthree.bandcamp.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fpelicanthree.bandcamp.com%2F%3Ffbclid%3DIwAR239gGXKf1JlQKeI4EuVuuuqXl655HmQMKE2Kn8vVDw56CDWGH1Oo6e2dQ&h=AT0rVc4qLafqU2Ob_4o3x5Hnpx4bWNz_Kl0K3Pi4pkLpktY7nPEC5XaoTUhtVjYcPj_60KbkkCQRCyBe7K72raTeFooBRN2kY2izQHhf67ATsAtaWepgidz2if2p_bMcUq3KXFKtyw&__tn__=-UK-R&c[0]=AT1BLqQIUjzSZsuF4mgjal-GUqJgJhDKJkMoDQ1fd4IrzCzzr4PrxLBsXdGrrU8PBHN2gJgmwKG8TyBfZNJhUB9kAIzZSytYg4pUvW9AqK29CTiZsdAwSkexG13sigLQoB-0g_GPW9EF08-XOKr9BKA23RGXbIE8SDBhCh7iiSJTUNGucQkTjGauD2mm)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://pelicanthree.bandcamp.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fpelicanthree.bandcamp.com%2F%3Ffbclid%3DIwAR239gGXKf1JlQKeI4EuVuuuqXl655HmQMKE2Kn8vVDw56CDWGH1Oo6e2dQ&h=AT0rVc4qLafqU2Ob_4o3x5Hnpx4bWNz_Kl0K3Pi4pkLpktY7nPEC5XaoTUhtVjYcPj_60KbkkCQRCyBe7K72raTeFooBRN2kY2izQHhf67ATsAtaWepgidz2if2p_bMcUq3KXFKtyw&__tn__=-UK-R&c[0]=AT1BLqQIUjzSZsuF4mgjal-GUqJgJhDKJkMoDQ1fd4IrzCzzr4PrxLBsXdGrrU8PBHN2gJgmwKG8TyBfZNJhUB9kAIzZSytYg4pUvW9AqK29CTiZsdAwSkexG13sigLQoB-0g_GPW9EF08-XOKr9BKA23RGXbIE8SDBhCh7iiSJTUNGucQkTjGauD2mm)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://pelicanthree.bandcamp.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fpelicanthree.bandcamp.com%2F%3Ffbclid%3DIwAR2VIhsEaMigNyPuxqravnHZINRKy2lFMj0ST21WgfvzwP6bUPrHXSnZ3pI&h=AT3vVaZeASF3rAf4UayKhFmJknk-N4gDprbcbufCw3LXFc2oKyM8ty6KCyWB_D2gK3bgS7896j45ixenA6YvOj4Y6kgK-BHqPHHAhtnzhFyIGdpj19vamVLgkFqD75V73ic4E3vUbQ)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n",
      "URL parse error with https://liveatbirds.com](https://l.facebook.com/l.php?u=https%3A%2F%2Fliveatbirds.com%2F%3Ffbclid%3DIwAR10UBrEEb3NAd5dlvZrbvx-uTr_ShK5aST3XvheozUS92tiu0cD8l4SbKM&h=AT3Rqcc8M8hEsOYa1xXbBHeTkUbO4-vKRoBJyP3HUfCeCQxJMQ-lvlkInfmYDoDfPxArVeuKJJcKojvegIrQXY-lxNv7Fvt-ZD6E6i7NzHCy1j0YMBlx2sm876nsS4fEkvWpo3Y)\n"
     ]
    }
   ],
   "source": [
    "print(\"Now processing text!\")\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.A.apply(str) #Change to string\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "health_liwc_labeled['processed_text_bert'] = health_liwc_labeled['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(str.lower) #Convert to lowercase\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters\n",
    "\n",
    "print(\"Now processing text!\")\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.A.apply(str) #Change to string\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "nonhealth_liwc_labeled['processed_text_bert'] = nonhealth_liwc_labeled['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(str.lower) #Convert to lowercase\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tokenizing!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now tokenizing!\")\n",
    "health_liwc_labeled['tokens'] = health_liwc_labeled.processed_text.apply(word_tokenize)\n",
    "nonhealth_liwc_labeled['tokens'] = nonhealth_liwc_labeled.processed_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "health_liwc_labeled['tokens'] = health_liwc_labeled.tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "nonhealth_liwc_labeled['tokens'] = nonhealth_liwc_labeled.tokens.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "health_liwc_labeled['tokens'] = health_liwc_labeled.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])\n",
    "nonhealth_liwc_labeled['tokens'] = nonhealth_liwc_labeled.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Back to Final String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_liwc_labeled['final_text'] = health_liwc_labeled.tokens.apply(lambda x: ' '.join(x))\n",
    "nonhealth_liwc_labeled['final_text'] = nonhealth_liwc_labeled.tokens.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n",
      "Compiling Rankings\n",
      "               term          rank\n",
      "4           com url  13153.893147\n",
      "10    give headache  12207.176996\n",
      "24      sore throat   6943.649977\n",
      "21  severe headache   6591.665318\n",
      "9      get headache   6077.591715\n",
      "['back pain', 'bite ly', 'blood pressure', 'body ache', 'com url', 'cough shortness', 'difficulty breathe', 'fever cough', 'fever headache', 'get headache', 'give headache', 'headache sore', 'include fever', 'loss taste', 'ly url', 'migraine headache', 'migraine headaches', 'muscle pain', 'nausea vomit', 'pain headache', 'runny nose', 'severe headache', 'shortness breath', 'side effect', 'sore throat', 'symptoms include', 'taste smell']\n",
      "Now running sentiment analysis!\n",
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(health_liwc_labeled['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(health_liwc_labeled['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]\n",
    "\n",
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in health_liwc_labeled['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)\n",
    "\n",
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            health_liwc_labeled.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/211214_feature_matrix_no_bert_health_comparator.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(file_path + file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonHealth Comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n",
      "Compiling Rankings\n",
      "                  term         rank\n",
      "7        bass clarinet  7968.855305\n",
      "61       play clarinet  5729.969439\n",
      "25             com url  3977.094907\n",
      "41      flute clarinet  3910.408249\n",
      "22  clarinet saxophone  2807.481308\n",
      "['ableton mix', 'alto flute', 'alto sax', 'alto saxophone', 'alto tenor', 'aquasonic voice', 'bandcamp com', 'bass clarinet', 'bass guitar', 'bass saxophones', 'bass tromboon', 'bob moor', 'brontosaurus tank', 'cello aquasonic', 'christmas flute', 'clarinet alto', 'clarinet bass', 'clarinet contrabass', 'clarinet contralto', 'clarinet flute', 'clarinet piano', 'clarinet player', 'clarinet saxophone', 'clarinets saxophones', 'clown headquarter', 'com url', 'contrabass clarinet', 'contralto clarinet', 'crotales cymbells', 'dahlman trumpet', 'double bass', 'double reeds', 'drum percussion', 'electric bass', 'english horn', 'eric dahlman', 'eric woods', 'evil clown', 'facebook com', 'flute alto', 'flute christmas', 'flute clarinet', 'flute sheng', 'french horn', 'game call', 'glynis lomon', 'gong brontosaurus', 'high school', 'jim warshauer', 'leap faith', 'log drum', 'lomon cello', 'moog subsequent', 'moor trumpet', 'new york', 'orchestral chime', 'overtone voice', 'pek alto', 'pek clarinet', 'pek clarinets', 'piano clarinet', 'play clarinet', 'russian wood', 'sax clarinet', 'saxophone clarinet', 'saxophone flute', 'saxophones clarinet', 'saxophones double', 'saxophones tarota', 'sheng melodica', 'soprano alto', 'tank bell', 'tenor bass', 'tenor sax', 'tenor saxophone', 'tenor saxophones', 'tibetan bowl', 'trumpet overtone', 'wind siren', 'wood block', 'wood flute', 'www facebook', 'www youtube', 'youtube com', 'yuri zbitnov', 'zbitnov drum']\n",
      "Now running sentiment analysis!\n",
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(nonhealth_liwc_labeled['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(nonhealth_liwc_labeled['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]\n",
    "\n",
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in nonhealth_liwc_labeled['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)\n",
    "\n",
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            nonhealth_liwc_labeled.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/211214_feature_matrix_no_bert_nonhealth_comparator.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(file_path + file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffe67d4efdf50a67adb541926f1b56bfeeb48b191d1b9a6489e883c5acad5bda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
