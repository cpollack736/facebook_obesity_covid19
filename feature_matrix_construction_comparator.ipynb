{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import preprocessor as p \n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replacement(text, token_type = \"url\"):\n",
    "    \"\"\"\n",
    "    Function that will take in a block of text and replace the url with a token of some type\n",
    "    text (str): A block of text that contains a url\n",
    "    token_type (str): A specfication on what token should replace the url. Default is \"url\", which just returns \"url\". Other options include: \n",
    "        \"domain\", which returns the domain (e.g., \".gov url\")\n",
    "        \"host\", which returns the host of the website and domain (e.g., \"cdc.gov url\")\n",
    "    \"\"\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # Get a list of all urls\n",
    "    \n",
    "    if urls == []: # If list is blank\n",
    "        return text #Return previous text since nothing needs to change\n",
    "    \n",
    "    new_text = text[:] #Deep copy of the text to a new object\n",
    "    \n",
    "    if token_type == \"url\": #For the base case\n",
    "        for url in urls:\n",
    "            new_text = new_text.replace(url, \"url\") #Replace each url with the \"url\" token\n",
    "\n",
    "    elif token_type == \"domain\": #For the case of just extracting the domain\n",
    "        for url in urls:\n",
    "            try: \n",
    "                urlparse(url).netloc.split(\".\")[-1] #Extract just the domain \n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc.split(\".\")[-1] #Extract just the domain\n",
    "                \n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "            \n",
    "    else: #Extracting full host name\n",
    "        for url in urls:\n",
    "            try:\n",
    "                urlparse(url).netloc #Extract the full host name\n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc #Extract the full host name\n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_score(sid, text, dataframe):\n",
    "    \"\"\"\n",
    "    Function that will take in a text and return an estimated valence. \n",
    "    Note that this assumes that the column names match the VADER output\n",
    "    (i.e., \"neg\", \"neu\", \"pos\", \"compound\")\n",
    "    param sid (str): Name of the SentimentIntensityAnalyzer() defined outside the function \n",
    "    param text (str): A string of text to analyze\n",
    "    param dataframe (DataFrame): The pandas dataframe to append results to\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return(dataframe.append(scores, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/catherinepollack/Documents/dartmouth/research/aim3_facebook_covid19_obesity/data\"\n",
    "file_name = \"/211118_tokenized_sentences_expanded_health_liwc.csv\"\n",
    "health_liwc_labeled = pd.read_csv(str(file_path + file_name))\n",
    "\n",
    "file_name = \"/211118_tokenized_sentences_expanded_nonhealth_liwc.csv\"\n",
    "nonhealth_liwc_labeled = pd.read_csv(str(file_path + file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing text!\n",
      "Now processing text!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now processing text!\")\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.A.apply(str) #Change to string\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "health_liwc_labeled['processed_text_bert'] = health_liwc_labeled['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(str.lower) #Convert to lowercase\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "health_liwc_labeled['processed_text'] = health_liwc_labeled.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters\n",
    "\n",
    "print(\"Now processing text!\")\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.A.apply(str) #Change to string\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "nonhealth_liwc_labeled['processed_text_bert'] = nonhealth_liwc_labeled['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(str.lower) #Convert to lowercase\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "nonhealth_liwc_labeled['processed_text'] = nonhealth_liwc_labeled.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tokenizing!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now tokenizing!\")\n",
    "health_liwc_labeled['tokens'] = health_liwc_labeled.processed_text.apply(word_tokenize)\n",
    "nonhealth_liwc_labeled['tokens'] = nonhealth_liwc_labeled.processed_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "health_liwc_labeled['tokens'] = health_liwc_labeled.tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "nonhealth_liwc_labeled['tokens'] = nonhealth_liwc_labeled.tokens.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "health_liwc_labeled['tokens'] = health_liwc_labeled.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])\n",
    "nonhealth_liwc_labeled['tokens'] = nonhealth_liwc_labeled.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Back to Final String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_liwc_labeled['final_text'] = health_liwc_labeled.tokens.apply(lambda x: ' '.join(x))\n",
    "nonhealth_liwc_labeled['final_text'] = nonhealth_liwc_labeled.tokens.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n",
      "Compiling Rankings\n",
      "                 term         rank\n",
      "135  fatigue headache  3010.511347\n",
      "193  headache fatigue  2833.269147\n",
      "377       side effect  2215.080029\n",
      "394       sore throat  2165.838842\n",
      "371  shortness breath  1781.698213\n",
      "['abdominal pain', 'abs lats', 'account sign', 'ache fatigue', 'ache headache', 'acid reflux', 'alert tv', 'alignment bone', 'allergic reactions', 'always perfect', 'always protect', 'anxiety stomachache', 'arrest never', 'art heaven', 'arteries hair', 'associate favor', 'atv snowmobile', 'back pain', 'bad always', 'bandage place', 'bank remind', 'befriend us', 'belly fat', 'belt truth', 'bible verse', 'biceps triceps', 'bite ly', 'bladder focus', 'blank kick', 'bless israel', 'bless miracles', 'bloat sickness', 'blood cells', 'blood circulation', 'blood cover', 'blood pressure', 'boat atv', 'body ache', 'body produce', 'bone bowels', 'bowels shield', 'bread forgive', 'breastplate righteousness', 'breath difficulty', 'breath fatigue', 'breath root', 'breathe fatigue', 'breathe legs', 'brush listerine', 'brush tick', 'bug infections', 'burn belly', 'car videos', 'care image', 'care massage', 'caregivers pay', 'cars truck', 'catheter site', 'cells immunity', 'channel bite', 'checklist haircut', 'chest comfort', 'chest pain', 'chill cough', 'cholesterol level', 'christ always', 'christ mind', 'chronic fatigue', 'circulate temp', 'clarity bladder', 'click bite', 'click free', 'click view', 'clot sores', 'cold food', 'collagen elastin', 'colon gum', 'colon gut', 'colostomy supply', 'come thy', 'comfort shave', 'command always', 'command never', 'compel help', 'computer always', 'confuse arrest', 'congestion runny', 'connection joke', 'constipation heal', 'content return', 'contortion bloat', 'cough shortness', 'cough sore', 'cover us', 'daily bread', 'debts forgive', 'debts lead', 'deliver us', 'depression anxiety', 'difficulty breathe', 'digestion clarity', 'discern understand', 'discomfort drainage', 'disease germ', 'do earth', 'drainage wrinkle', 'drug get', 'dry cough', 'dvd bite', 'earth heaven', 'easy pray', 'eat foods', 'effect may', 'egypt jordan', 'elastin caregivers', 'enamel breath', 'enemies phone', 'energy prayers', 'everything bank', 'evil confuse', 'evil thy', 'excess fat', 'expand never', 'eye bandage', 'facebook market', 'facebook page', 'faith sword', 'family friends', 'fast natural', 'fat cholesterol', 'fat fast', 'fat varicose', 'father art', 'father daughter', 'fatigue cough', 'fatigue headache', 'fatigue muscle', 'favor bless', 'favor workout', 'fever chill', 'fever cough', 'fever fatigue', 'fever headache', 'fight us', 'finance self', 'finger hand', 'flower flower', 'flower growth', 'flu facebook', 'flyers vintage', 'focus time', 'follow symptoms', 'food teeth', 'foods bite', 'foods lose', 'forearm wrists', 'forever amen', 'forgive others', 'forgive us', 'free immune', 'free muscle', 'free video', 'free vintage', 'free warn', 'friends associate', 'friends never', 'gap teeth', 'gas christ', 'germ gap', 'germ patch', 'get colostomy', 'get drug', 'get everything', 'get godly', 'give us', 'glory forever', 'go blank', 'god fight', 'godly care', 'grind disease', 'group samsung', 'growth style', 'growth throat', 'gum eye', 'gum teeth', 'gut colon', 'hair growth', 'hair hair', 'haircut never', 'hallow thy', 'hamas palestinian', 'hand contortion', 'headache click', 'headache fatigue', 'headache loss', 'headache muscle', 'headache new', 'headache sore', 'headache withdrawal', 'heal clot', 'health wellness', 'hear vision', 'heart disease', 'heart diseases', 'heart metabolism', 'heaven give', 'heaven hallow', 'helmet salvation', 'help chest', 'help prevent', 'help supplement', 'help walk', 'high blood', 'high cholesterol', 'high voice', 'hormonal imbalance', 'humid sick', 'ijna command', 'image like', 'immune god', 'immune system', 'immunity alert', 'include allergic', 'include fever', 'infection germ', 'infections tick', 'injection site', 'innovative pivot', 'internet connection', 'iran iraq', 'iraq enemies', 'israel always', 'israel jews', 'itch mites', 'jesus blood', 'jesus body', 'jew israel', 'jews messianic', 'joint pain', 'joke befriend', 'jordan russia', 'juice tough', 'kick group', 'kingdom come', 'kingdom power', 'knowledge discern', 'lancaster nh', 'lats shrug', 'lead us', 'leak catheter', 'lebanon syria', 'legs alignment', 'like never', 'like videos', 'listerine miralax', 'lose tv', 'lose weight', 'loss appetite', 'loss smell', 'loss taste', 'love jesus', 'lower cholesterol', 'ly url', 'manage energy', 'market headache', 'massage remind', 'may include', 'media protect', 'meek love', 'memory fat', 'memory gut', 'messianic jew', 'metabolism digestion', 'mind christ', 'mind rachel', 'ministry expand', 'miracles finance', 'miracles picture', 'miralax prune', 'mites bug', 'mouth gas', 'muscle ache', 'muscle body', 'muscle car', 'muscle pain', 'music content', 'must contact', 'name thy', 'natural foods', 'naturally bite', 'nausea vomit', 'neck injection', 'nerve pain', 'never anxiety', 'never chest', 'never evil', 'never infection', 'never leak', 'never lose', 'never pain', 'new loss', 'nh vintage', 'nose breathe', 'nose nausea', 'numb always', 'numb humid', 'order lancaster', 'order washington', 'others debts', 'page go', 'pain fatigue', 'pain headache', 'pain numb', 'pain pressure', 'palestinian lebanon', 'password social', 'patch stay', 'patio never', 'pay time', 'peace jesus', 'pecs biceps', 'perfect bless', 'perfect father', 'perfect meek', 'perfect property', 'phone tablet', 'picture scripture', 'pivot ramp', 'place relax', 'power glory', 'pray thank', 'pray us', 'pray viral', 'prayers wisdom', 'pressure constipation', 'pressure headache', 'produce collagen', 'property password', 'protect bad', 'protect hamas', 'protect relax', 'prune juice', 'rachel must', 'rally dvd', 'ramp system', 'reactions neck', 'read checklist', 'reduce fatigue', 'reduce risk', 'reflux finger', 'relax high', 'relax mouth', 'relieve headache', 'remind brush', 'remind read', 'return say', 'righteousness belt', 'root white', 'runny nose', 'russia iran', 'sale bite', 'salvation breastplate', 'samsung account', 'sandals peace', 'say mind', 'scar pressure', 'scripture skin', 'sediment scar', 'self talk', 'shave brush', 'shield faith', 'shortness breath', 'shoulder abs', 'show dvd', 'shrug forearm', 'sick always', 'sickness itch', 'side effect', 'sign fat', 'site pain', 'site syringe', 'situation turnaround', 'skin vein', 'smell sore', 'smell taste', 'snow flyers', 'snowmobile bite', 'snowmobile rally', 'snowmobile sale', 'snowmobile show', 'snowmobile subscribe', 'snowmobile videos', 'social media', 'soft tone', 'sore throat', 'sores colon', 'spirit helmet', 'stay help', 'stomachache flu', 'stomachache nerve', 'style flower', 'subscribe health', 'subscribe youtube', 'supplement help', 'supply get', 'sword spirit', 'symptoms covid', 'symptoms fever', 'symptoms headache', 'symptoms include', 'syria egypt', 'syringe get', 'system atv', 'tablet computer', 'talk think', 'taste smell', 'teeth blood', 'teeth discomfort', 'teeth enamel', 'teeth grind', 'temp memory', 'temptation deliver', 'thank bible', 'think evil', 'throat congestion', 'throat nose', 'throat runny', 'thy do', 'thy kingdom', 'thy name', 'tick stomachache', 'tick water', 'time get', 'time manage', 'tone favor', 'tough situation', 'triceps shoulder', 'truck boat', 'truth sandals', 'turnaround ministry', 'tv internet', 'tv music', 'understand hear', 'url burn', 'url click', 'url eat', 'url facebook', 'url free', 'url innovative', 'url insomnia', 'url like', 'url order', 'url snowmobile', 'url subscribe', 'us daily', 'us debts', 'us evil', 'us family', 'us free', 'us heart', 'us never', 'us temptation', 'us us', 'varicose vein', 'vein arteries', 'vein cold', 'verse easy', 'video bite', 'videos bite', 'videos cars', 'view free', 'vintage snowmobile', 'viral pray', 'vision circulate', 'voice soft', 'vomit diarrhea', 'walk protect', 'warn bad', 'washington snow', 'water patio', 'weight loss', 'weight naturally', 'wellness youtube', 'white teeth', 'wisdom knowledge', 'withdrawal acid', 'workout pecs', 'wrinkle sediment', 'wrists gum', 'youtube channel']\n",
      "Now running sentiment analysis!\n",
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(health_liwc_labeled['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(health_liwc_labeled['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]\n",
    "\n",
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in health_liwc_labeled['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)\n",
    "\n",
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            health_liwc_labeled.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/211118_feature_matrix_no_bert_health_comparator.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(file_path + file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonHealth Comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n",
      "Compiling Rankings\n",
      "             term        rank\n",
      "72  play clarinet  131.286987\n",
      "20        com url  112.374722\n",
      "33    father pass   92.648203\n",
      "73  radu malfatti   79.000000\n",
      "40   found father   77.238876\n",
      "['ago father', 'albums last', 'alone second', 'alto flute', 'alto sax', 'arrive days', 'arrive hours', 'batch pek', 'bell chime', 'bell crank', 'block cow', 'brontosaurus bell', 'canvas cm', 'category first', 'chime wood', 'chunk show', 'clarinet contrabass', 'clarinet flute', 'clarinets flute', 'colorful musical', 'com url', 'come phrase', 'composition musicians', 'contrabass clarinet', 'cow bell', 'crank siren', 'days ago', 'disappoint mike', 'drive mount', 'early age', 'facebook com', 'fall category', 'father clarinets', 'father pass', 'father play', 'father teach', 'first tenor', 'flute alto', 'flute gong', 'flute ronin', 'found father', 'frantic drive', 'gong brontosaurus', 'gong one', 'gong plate', 'great grandfather', 'hebrew musical', 'hours frantic', 'klezmer come', 'kli zemr', 'know father', 'last months', 'live mother', 'los angeles', 'mean hebrew', 'mike miss', 'miss chunk', 'mix technics', 'months fall', 'mother father', 'mount stage', 'music director', 'musical composition', 'musical instrument', 'musicians zfat', 'news arrive', 'one alto', 'pass away', 'pek clarinet', 'pek solo', 'phrase kli', 'plate gong', 'play clarinet', 'radu malfatti', 'raymond sheldon', 'requiem father', 'reshart gallery', 'ronin gong', 'sad since', 'sax alone', 'second requiem', 'sheldon peck', 'show arrive', 'since news', 'singer songwriter', 'siren sad', 'solo albums', 'stage frenzy', 'technics canvas', 'tenor sax', 'url colorful', 'wood block', 'www reshart', 'year old', 'zemr mean', 'zfat klezmer', 'zfat mix']\n",
      "Now running sentiment analysis!\n",
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(nonhealth_liwc_labeled['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(nonhealth_liwc_labeled['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]\n",
    "\n",
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in nonhealth_liwc_labeled['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)\n",
    "\n",
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            nonhealth_liwc_labeled.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/211118_feature_matrix_no_bert_nonhealth_comparator.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(file_path + file_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffe67d4efdf50a67adb541926f1b56bfeeb48b191d1b9a6489e883c5acad5bda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
