{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import gensim\n",
    "import gensim.corpora as corpora \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors._nearest_centroid import NearestCentroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (84) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (97,98,99,100) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/catherinepollack/Documents/dartmouth/research/aim3_facebook_covid19_obesity/data\"\n",
    "dat_obesity = \"/211105_feature_matrix_bert.csv\"\n",
    "dat_health = \"/211214_feature_matrix_bert_health_comparator.csv\"\n",
    "dat_nonhealth = \"/211214_feature_matrix_bert_nonhealth_comparator.csv\"\n",
    "\n",
    "dat_obesity = pd.read_csv(str(file_path + dat_obesity))\n",
    "dat_health = pd.read_csv(str(file_path + dat_health))\n",
    "dat_nonhealth = pd.read_csv(str(file_path + dat_nonhealth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matrix Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obesity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629960, 938)\n"
     ]
    }
   ],
   "source": [
    "#api whatsapp : without skip\n",
    "#neg: compound\n",
    "#WC: OtherP\n",
    "#BERT 0 : BERT 767\n",
    "dat_obesity_1 = dat_obesity.loc[:,\"api whatsapp\":\"without skip\"]\n",
    "dat_obesity_2 = dat_obesity.loc[:,\"neg\":\"compound\"]\n",
    "dat_obesity_3 = dat_obesity.loc[:,\"WC\":\"OtherP\"]\n",
    "dat_obesity_4 = dat_obesity.loc[:,\"BERT 0\":\"BERT 767\"]\n",
    "dat_obesity_fm = pd.concat([dat_obesity_1,\n",
    "                            dat_obesity_2,\n",
    "                            dat_obesity_3,\n",
    "                            dat_obesity_4],\n",
    "                            axis = 1)\n",
    "dat_obesity_fm.head()\n",
    "print(dat_obesity_fm.shape) #629960                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Randomly Sample and Run Hierarchical Clustering, then run KMeans and DBSCAN\n",
    "- LDA\n",
    "- Pick Features Based on Reddit Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Sample and Run Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Sample and Run Initial Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "## Randomly sample 1% of the data frame\n",
    "dat_obesity_fm_1 = dat_obesity_fm.sample(frac = 0.01,\n",
    "                                          random_state = 110295)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run hierarchical clustering - Note: takes a long time\n",
    "plt.figure(figsize=(14, 7))\n",
    "dend = shc.dendrogram(shc.linkage(dat_obesity_fm_1, \n",
    "                                  method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Assign Labels to Sample and Identify Centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'C1': 2, 'C2': 4380, 'C3': 731, 'C4': 1047, 'C5': 140})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(dend['leaves_color_list']) #4 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Sampled Data to Find Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "model_agg = AgglomerativeClustering(n_clusters = 4,\n",
    "                                    affinity=\"euclidean\",\n",
    "                                    linkage = \"ward\")\n",
    "\n",
    "yhat_agg = model_agg.fit_predict(dat_obesity_fm_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "find_center = NearestCentroid()\n",
    "find_center.fit(dat_obesity_fm_1, yhat_agg)\n",
    "centoids = find_center.centroids_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Run K-Means to Get Full Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans with Centers from Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1146: RuntimeWarning: Explicit initial center position passed: performing only one init in KMeans instead of n_init=10.\n",
      "  self._check_params(X)\n"
     ]
    }
   ],
   "source": [
    "model_kmeans = KMeans(n_clusters = 4,\n",
    "                      random_state = 110295,\n",
    "                      init = centoids)\n",
    "yhat_kmeans_centroids = model_kmeans.fit_predict(dat_obesity_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means ++ with Same # of Clusters but No Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "model_kmeans = KMeans(n_clusters = 4,\n",
    "                      random_state = 110295,\n",
    "                      init = \"k-means++\")\n",
    "yhat_kmeans_plus = model_kmeans.fit_predict(dat_obesity_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "yhat_kmeans = dict({\"kmeans_centroids\": yhat_kmeans_centroids,\n",
    "                   \"kmeans_plus\": yhat_kmeans_plus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(yhat_kmeans).to_csv(\"220108_kmeans_centroids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Terms and Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "texts = [ast.literal_eval(x) for x in dat_obesity.tokens]\n",
    "\n",
    "terms = [item for sublist in texts for item in sublist]\n",
    "terms = list(set(terms)) #106269"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA on Initial Cluster Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.031*\"obesity\" + 0.029*\"fat\" + 0.011*\"diabetes\" + 0.011*\"body\" + 0.011*\"high\" + 0.009*\"diet\" + 0.008*\"disease\" + 0.008*\"health\" + 0.008*\"blood\" + 0.008*\"weight\"'),\n",
       " (1,\n",
       "  '0.046*\"obesity\" + 0.030*\"obese\" + 0.011*\"weight\" + 0.010*\"health\" + 0.009*\"overweight\" + 0.009*\"people\" + 0.007*\"new\" + 0.006*\"children\" + 0.006*\"url\" + 0.005*\"say\"'),\n",
       " (2,\n",
       "  '0.051*\"obesity\" + 0.026*\"risk\" + 0.024*\"disease\" + 0.024*\"diabetes\" + 0.016*\"heart\" + 0.011*\"cancer\" + 0.011*\"health\" + 0.010*\"increase\" + 0.010*\"condition\" + 0.009*\"high\"'),\n",
       " (3,\n",
       "  '0.041*\"fat\" + 0.034*\"obesity\" + 0.028*\"reduce\" + 0.021*\"weight\" + 0.017*\"slim\" + 0.015*\"pregnancy\" + 0.015*\"thyroid\" + 0.014*\"get\" + 0.014*\"whatsapp\" + 0.014*\"pcod\"')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(texts) \n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts] \n",
    "\n",
    "# number of topics\n",
    "num_topics = 4\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state = 110295)\n",
    "                                       \n",
    "# Print the Keyword in the 4 topics\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.7029355185845132\n"
     ]
    }
   ],
   "source": [
    "#Compute Perplexity\n",
    "#print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda) #0.703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Best # of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence of 0.6273890024017148 for 2 topics.\n",
      "Coherence of 0.6985926613827784 for 3 topics.\n",
      "Coherence of 0.6995046887705996 for 4 topics.\n",
      "Coherence of 0.6777021835542495 for 5 topics.\n",
      "Coherence of 0.6740787489713401 for 6 topics.\n",
      "Coherence of 0.6496037708484023 for 7 topics.\n",
      "Coherence of 0.6492491653002559 for 8 topics.\n",
      "Coherence of 0.6390425753654293 for 9 topics.\n",
      "Coherence of 0.6262009945113853 for 10 topics.\n",
      "Coherence of 0.6596413041744863 for 11 topics.\n",
      "Coherence of 0.6410610996106468 for 12 topics.\n",
      "Coherence of 0.6384299803411456 for 13 topics.\n",
      "Coherence of 0.6641276460072927 for 14 topics.\n",
      "Coherence of 0.648647153958825 for 15 topics.\n",
      "Coherence of 0.6532052388324132 for 16 topics.\n",
      "Coherence of 0.6376812437094737 for 17 topics.\n",
      "Coherence of 0.6415805258418805 for 18 topics.\n",
      "Coherence of 0.6378327408109127 for 19 topics.\n",
      "Coherence of 0.626787744898942 for 20 topics.\n",
      "Coherence of 0.6157198329797225 for 21 topics.\n",
      "Coherence of 0.6180342286344714 for 22 topics.\n",
      "Coherence of 0.6184446848218134 for 23 topics.\n",
      "Coherence of 0.6117732478605302 for 24 topics.\n",
      "Coherence of 0.617452715592079 for 25 topics.\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 26):\n",
    "    lda_model_loop = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=i,\n",
    "                                           random_state = 110295)\n",
    "\n",
    "    coherence_model_lda_loop = CoherenceModel(model=lda_model_loop, \n",
    "                                         texts=texts, \n",
    "                                         dictionary=id2word, \n",
    "                                         coherence='c_v')\n",
    "    \n",
    "    coherence_lda_loop = coherence_model_lda_loop.get_coherence()                                       \n",
    "    print(f\"Coherence of {coherence_lda_loop} for {i} topics.\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el44071405931475259367043550536\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el44071405931475259367043550536_data = {\"mdsDat\": {\"x\": [-0.07504950449784667, -0.010970965197928133, -0.12554931693325716, 0.21156978662903184], \"y\": [-0.12713518339100172, 0.10135413013958434, 0.04118417399688735, -0.015403120745469791], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [33.74362089120014, 29.735448752444643, 18.701595964458015, 17.819334391897204]}, \"tinfo\": {\"Term\": [\"fat\", \"risk\", \"reduce\", \"disease\", \"diabetes\", \"slim\", \"obese\", \"thyroid\", \"pcod\", \"pregnancy\", \"whatsapp\", \"post\", \"weight\", \"belly\", \"heart\", \"pcos\", \"remove\", \"effect\", \"side\", \"get\", \"without\", \"obesity\", \"cancer\", \"fit\", \"covid\", \"detail\", \"skip\", \"high\", \"arm\", \"diet\", \"pelosi\", \"oildok\", \"turtle\", \"hancock\", \"konkol\", \"module\", \"flail\", \"soccer\", \"obeseeither\", \"webinar\", \"nicely\", \"argentine\", \"monarch\", \"frugal\", \"locsin\", \"rand\", \"devdiscourse\", \"baptists\", \"protesters\", \"johnson\", \"viii\", \"inequities\", \"britons\", \"behead\", \"cosmopolitan\", \"maradona\", \"pinterest\", \"cancercarepune\", \"ocw\", \"rome\", \"boris\", \"nancy\", \"wood\", \"robert\", \"cooper\", \"stifle\", \"glenn\", \"veterinarians\", \"semaglutide\", \"unite\", \"speaker\", \"trump\", \"pandemic\", \"donald\", \"president\", \"nationwide\", \"survey\", \"morbidly\", \"anderson\", \"thirds\", \"obese\", \"million\", \"world\", \"percent\", \"third\", \"severely\", \"pet\", \"overweight\", \"children\", \"dog\", \"country\", \"uk\", \"foundation\", \"hospitalize\", \"consider\", \"cat\", \"hospital\", \"highest\", \"childhood\", \"state\", \"report\", \"www\", \"accord\", \"adults\", \"new\", \"program\", \"live\", \"us\", \"obesity\", \"one\", \"people\", \"likely\", \"say\", \"health\", \"weight\", \"epidemic\", \"url\", \"study\", \"com\", \"covid\", \"find\", \"patients\", \"time\", \"healthy\", \"help\", \"food\", \"loss\", \"get\", \"body\", \"fat\", \"ating\", \"softgels\", \"diabetwatch\", \"kapag\", \"lishou\", \"gaya\", \"naman\", \"bato\", \"nagkakaroon\", \"alak\", \"kondisyon\", \"palainom\", \"katawan\", \"problema\", \"loob\", \"superfoods\", \"naughton\", \"ilalabas\", \"kino\", \"klapers\", \"kaya\", \"sakit\", \"softgel\", \"pagtaba\", \"magkaroon\", \"tinatawag\", \"okinawa\", \"ayaw\", \"irritableness\", \"binder\", \"ay\", \"uncontrollable\", \"isang\", \"pag\", \"ang\", \"dosage\", \"urinatary\", \"lahat\", \"ng\", \"mataas\", \"trans\", \"faty\", \"insomnia\", \"carbohydrates\", \"satiety\", \"favourite\", \"carb\", \"sa\", \"composition\", \"anemia\", \"salt\", \"inflammation\", \"cholesterol\", \"omega\", \"calories\", \"low\", \"process\", \"anxiety\", \"brain\", \"insulin\", \"arthritis\", \"burn\", \"foods\", \"energy\", \"disorder\", \"stress\", \"sugar\", \"metabolism\", \"blood\", \"protein\", \"fatty\", \"contain\", \"fat\", \"high\", \"eat\", \"diseases\", \"depression\", \"sleep\", \"level\", \"body\", \"lead\", \"cause\", \"prevent\", \"diet\", \"pressure\", \"diabetes\", \"obesity\", \"like\", \"help\", \"liver\", \"may\", \"disease\", \"health\", \"food\", \"weight\", \"heart\", \"increase\", \"problems\", \"cancer\", \"reduce\", \"obese\", \"ija\", \"baba\", \"tunde\", \"betja\", \"ifa\", \"approval\", \"obesitycareweek\", \"lotto\", \"cov\", \"hotline\", \"duo\", \"drucker\", \"glory\", \"ogbu\", \"ekpakperan\", \"intensify\", \"finland\", \"edo\", \"furnace\", \"daycares\", \"onor\", \"worldobesity\", \"adjunct\", \"antiviral\", \"rewire\", \"fluke\", \"liquor\", \"ifaponmile\", \"ijebu\", \"anecdotal\", \"ring\", \"visa\", \"luck\", \"favour\", \"boom\", \"command\", \"marriage\", \"breakthrough\", \"caesar\", \"spiritual\", \"womb\", \"success\", \"political\", \"land\", \"financial\", \"recover\", \"immunocompromised\", \"ritual\", \"impotence\", \"ohioans\", \"epilepsy\", \"court\", \"promotion\", \"protection\", \"promise\", \"attack\", \"risk\", \"stroke\", \"disease\", \"severe\", \"heart\", \"diabetes\", \"cancer\", \"condition\", \"hypertension\", \"factor\", \"covid\", \"increase\", \"chronic\", \"love\", \"type\", \"obesity\", \"include\", \"higher\", \"kidney\", \"high\", \"health\", \"study\", \"ly\", \"age\", \"people\", \"link\", \"back\", \"blood\", \"say\", \"get\", \"problems\", \"pressure\", \"reduce\", \"help\", \"url\", \"gaikwad\", \"herbalifechange\", \"pallavi\", \"urlstruggling\", \"pareshan\", \"apko\", \"ager\", \"karti\", \"rm\", \"bdi\", \"hui\", \"mai\", \"tush\", \"lossing\", \"whtasp\", \"damini\", \"vlckd\", \"c_\", \"xxl\", \"agar\", \"helo\", \"atin\", \"proteect\", \"vishwakarma\", \"mymi\", \"cinderella\", \"jel\", \"enhancers\", \"symptomps\", \"bhi\", \"slim\", \"api\", \"exhaustion\", \"formula\", \"pcod\", \"intensive\", \"ur\", \"skip\", \"forever\", \"wmc\", \"arm\", \"plz\", \"cappuccino\", \"side\", \"detail\", \"pcos\", \"remove\", \"relieve\", \"figure\", \"thyroid\", \"whatsapp\", \"mcg\", \"belly\", \"post\", \"inch\", \"click\", \"meals\", \"pregnancy\", \"fit\", \"effect\", \"reduce\", \"tummy\", \"without\", \"info\", \"month\", \"skin\", \"fat\", \"get\", \"weight\", \"exercise\", \"obesity\", \"loss\", \"diet\", \"lose\", \"call\", \"body\", \"url\", \"com\"], \"Freq\": [190312.0, 73210.0, 80933.0, 87048.0, 97041.0, 33291.0, 139526.0, 30804.0, 28081.0, 35250.0, 29959.0, 29155.0, 107586.0, 27848.0, 53897.0, 23857.0, 22656.0, 33701.0, 21777.0, 54237.0, 28325.0, 442231.0, 41283.0, 20390.0, 32952.0, 16224.0, 15612.0, 59701.0, 14690.0, 61113.0, 2972.1691061517727, 741.904032805999, 1184.4932338562003, 415.5108804319905, 291.257603890504, 281.24624274322, 690.3740494803455, 396.0710525790384, 187.92127749544767, 768.7249851947093, 186.7889033749737, 350.993249828793, 455.007292679863, 238.22608448324806, 237.64056023880258, 180.9853623119218, 187.10323259153623, 184.1915528545028, 183.48198053947064, 3794.7128706977214, 466.39336292280154, 147.28723152144195, 163.42496236264017, 452.2060646616734, 124.97274450634139, 358.9543678896437, 159.3037224365151, 121.53133572304135, 122.25278281786083, 157.08096970673287, 1832.2162734820176, 2186.6891862363022, 2390.055688864939, 2600.3765830753327, 1111.991359194073, 452.66134588762947, 282.18283210973453, 709.5716643592792, 358.2076936656185, 5929.343673019203, 1228.6496296067824, 6583.3878910835965, 5590.662373808959, 1836.4877239501786, 4239.677244596345, 2474.526040206972, 2261.766424098335, 9189.579537553967, 903.3399288872062, 1152.249465364606, 113450.5850576663, 5426.564846150487, 11224.132949192097, 7954.13829302717, 2060.7121959411647, 3008.086031412106, 7575.599376794838, 34634.01526139052, 23437.434403996507, 8821.889506891726, 4299.425931103305, 6235.865248982923, 3314.660745659806, 2395.878343443402, 8068.053760422698, 4709.546773077988, 3702.839597278951, 4147.09039993556, 18502.54400159963, 13311.49150030318, 11048.912819578296, 13975.589915478475, 11638.39554084384, 13079.913557373202, 24249.354415783673, 7098.712992136391, 11590.98491320414, 13054.502613083076, 170043.70250338272, 18779.454756820218, 32721.845607595715, 9574.326740482787, 20365.297220202712, 38193.60532338942, 41036.22329319828, 8243.747579882262, 22229.405573492757, 20218.186158359484, 16888.739956295976, 15712.276567821997, 14302.37517958779, 10072.822838553682, 11762.38515931813, 12118.321435345933, 13413.84320954087, 11892.055585686301, 12370.543926938524, 12546.212587047305, 12305.204225393061, 11887.815801297573, 1227.2434207006675, 1974.2116778248158, 1061.770077305349, 700.6892463624245, 1856.103764374315, 664.3114291373987, 723.8360862792675, 634.9142266687473, 592.0759494670965, 595.6779950367628, 556.380363543307, 548.751156009024, 542.7275667977163, 623.9727713545442, 422.2636666004683, 788.7077396064246, 354.2404085550774, 347.9084821405986, 188.32409024478952, 480.77286081938763, 1277.1376648622258, 168.04728770479062, 1731.2958598144605, 163.1904272619213, 139.54123135785667, 200.27323196252237, 273.91639304829397, 121.62470399850936, 130.86250428485465, 514.9904063424807, 1148.5229688911447, 730.0288237470801, 714.7612823177817, 814.6460991395185, 2904.1698704191817, 825.5082841267324, 657.5212256245758, 759.4040464987845, 4736.379771192417, 746.2884149225802, 1942.0415907173253, 1398.28336632277, 1908.5036007038402, 2796.092161887459, 1366.727851213914, 1592.8216362048936, 2303.1584015012522, 3582.9656463326664, 1991.7186179370558, 1898.340561843513, 3743.2719405297016, 6840.8608354184735, 14871.60492431104, 3090.9896282214268, 8218.58313217723, 14747.616385373372, 6873.793714268009, 5450.009575907364, 7161.663787274101, 8331.897556053558, 7057.0005799880955, 13006.468069655444, 11489.391309405388, 7932.653390357621, 11101.463448761586, 8933.46062030287, 22220.348594559255, 7855.670100698802, 26269.197468067516, 4805.2183333905605, 12363.827016839972, 5093.601393877396, 95088.27072901359, 35440.44116465369, 19633.966632214495, 17964.086115237285, 9285.016063086896, 10702.959330578855, 14276.802438738665, 36219.43597177727, 17101.016406127932, 19813.501547308624, 12210.499569465557, 28299.285408930973, 14583.721854602753, 37697.92552321119, 101538.77561401353, 15774.395613375042, 21922.250116925916, 11207.760417964659, 16402.24030299777, 27098.136308189736, 27037.89888705287, 15094.044246885633, 24856.307624193625, 17758.39571923222, 16609.666141001773, 13599.454074968131, 14785.733419374816, 12437.033617743384, 12495.855927364812, 357.3904838742894, 3434.9509009547314, 474.59696842119905, 3057.198675837917, 290.2155020291673, 3186.4864377501535, 135.38476247934437, 3076.90655310724, 315.79213752363324, 193.10873831930652, 157.0151945646248, 131.4006939811693, 3137.6738903972505, 102.6871612513536, 86.45936363800615, 83.46952419072812, 90.31449549784023, 128.90601570157446, 77.69355045943844, 104.25112465908269, 66.91554588222785, 64.68720793393628, 144.4012816141782, 119.80312058894044, 71.48881469865798, 58.85133373548119, 103.65252568481058, 62.360007543975364, 62.30873565909259, 65.41216661688203, 9325.340069061232, 3049.134253329565, 3128.215792200819, 3012.0079542985222, 2630.2220754479513, 3065.968951687687, 3170.1412207963663, 3111.742434977626, 1083.1967437802557, 6490.586890580382, 2981.8615735445183, 6745.989440154162, 3499.401369440424, 3388.939889360528, 3342.923348261266, 3415.4755033111414, 2404.97191695976, 2905.8520353533177, 3149.1892048888153, 567.1288780896739, 3331.6679205074643, 3341.220741529685, 3474.0094013839266, 3558.221359296686, 3642.7475373447423, 6695.289648683367, 53816.42054576516, 9938.62949579496, 50527.009343330246, 11295.683512282712, 32212.69236371975, 50024.67640758581, 23176.129013461057, 20766.04759341371, 11467.863644935995, 12656.039845356881, 16795.332128715825, 21647.90652333252, 14983.230618787256, 7000.126935135796, 14829.684437355574, 104502.73733397182, 15560.588840822431, 8333.741139266069, 6729.227346601594, 17696.753005053055, 21826.860540096084, 14165.515870856536, 5014.781223450024, 10357.990044412863, 15889.180881007429, 10200.38664874761, 7371.131721601305, 10472.401754824106, 10219.829350187374, 11072.435755865594, 9187.1553335094, 8541.409154560146, 10779.001751323973, 9567.724721247037, 9204.567023699745, 2197.3241051594237, 2306.195876282118, 2195.7052562290714, 850.47483868355, 545.8829834411379, 582.9505464718856, 504.1158449267525, 477.5311705581289, 1202.0874175657082, 489.03904005212775, 537.6215165282835, 550.7604330148306, 198.78868419567877, 189.5989762470297, 684.6089677442455, 136.5140403908967, 279.92118854855227, 543.1492430796238, 2410.0768646617007, 144.49509504298038, 124.45128791973592, 127.34450505918961, 257.10528588935256, 99.48359411630385, 102.89817819582845, 97.76296581487975, 141.2136445084832, 606.2751884997276, 86.6964958003914, 587.563189286913, 32531.050326263703, 12214.275883476834, 9032.80776795984, 10013.108644242908, 27187.31774159115, 9864.855343417617, 3494.687322270253, 15127.107163334806, 12864.96839698981, 1330.9052538273588, 14113.361834832778, 2341.558803071857, 549.4970459251003, 20685.936138952464, 15475.873241365005, 22401.587058321496, 21286.158683351125, 9980.552989795247, 13871.925876120933, 28557.411417770276, 27723.557841892674, 1489.0431083202807, 25506.703780138512, 26664.36562611371, 6519.256975134947, 11526.752323350342, 13503.138024791104, 30139.775657753595, 17867.431958647387, 25171.700323268004, 54815.59279725519, 6988.460190751067, 20479.084492694106, 6166.194888495745, 9034.563372629744, 12976.919348432555, 80057.50081416487, 27951.238509735726, 40604.03896699391, 19295.806945178272, 66146.36063060243, 19080.058158343312, 22293.87026615831, 16322.609803494623, 15154.236569945735, 20269.505978221954, 17038.627120625304, 15332.462513963748], \"Total\": [190312.0, 73210.0, 80933.0, 87048.0, 97041.0, 33291.0, 139526.0, 30804.0, 28081.0, 35250.0, 29959.0, 29155.0, 107586.0, 27848.0, 53897.0, 23857.0, 22656.0, 33701.0, 21777.0, 54237.0, 28325.0, 442231.0, 41283.0, 20390.0, 32952.0, 16224.0, 15612.0, 59701.0, 14690.0, 61113.0, 2975.572795002656, 742.7754532919307, 1186.1792309145537, 416.7328440133108, 292.12055419335593, 282.1416278588783, 693.0564028733725, 397.65493077744395, 188.78351624838368, 772.2747024429826, 187.66286604804165, 352.646394244607, 457.1771429482159, 239.39531939565987, 238.81150821603362, 181.88241291243492, 188.0941018140868, 185.1935251388109, 184.5030732190791, 3816.376438953947, 469.25560408278443, 148.24277561005866, 164.4901841921238, 455.22121002300264, 125.87617409053419, 361.56890980074576, 160.48342847118028, 122.45115589086721, 123.189703034193, 158.32054555384553, 1848.3862611705263, 2220.6876367051077, 2437.200816735041, 2653.968636497559, 1126.5927002641815, 456.90357117986736, 284.5883950698527, 722.3396430834008, 362.40961882981105, 6179.14571979532, 1259.9431009446748, 6992.310306884362, 5947.36774623797, 1914.1580770794867, 4511.738471740059, 2608.6852244508773, 2393.708227506078, 10188.933392078538, 935.2090214849007, 1202.5706093486872, 139526.0747711875, 5999.576601307793, 12928.402565993189, 9101.760361775787, 2225.2082285719334, 3318.5301919464287, 8774.923776772563, 44143.94519043019, 29138.832954901052, 10312.576729097998, 4858.001602611046, 7217.589939160625, 3688.500131863689, 2614.6922256280977, 9703.799833906149, 5423.802995229828, 4179.155232096038, 4739.191490817879, 25099.15630629851, 17518.005152050915, 14213.038578938378, 18602.289128160428, 15302.960921117316, 17611.782931121652, 35904.59079140368, 8905.534003652681, 16001.164350187493, 18927.116524189998, 442231.5760819705, 29912.262054924864, 62162.6710533093, 13305.78505903871, 36936.49952123813, 88379.01057504017, 107586.06701835405, 11092.64092657924, 50048.512071083445, 43278.82146746735, 34826.811719703495, 32952.658181936735, 28175.444544768914, 15621.274158896375, 24382.596721993472, 29846.20802037534, 49396.448423884234, 33915.710737005626, 42629.92685804523, 54237.984815530814, 70558.4993272785, 190312.42156538652, 1228.211709813748, 1975.9004453725609, 1062.7227865733512, 701.5575712253666, 1858.4492128668594, 665.2325885030756, 724.863803050921, 635.8618033968571, 593.0612116211623, 596.7051593999623, 557.4511590578767, 549.821104498545, 543.8251443110215, 625.2903870306696, 423.1694137394227, 790.4385596764619, 355.2634979952816, 349.0339279148764, 189.16750436383305, 483.05299601670873, 1283.4508485405236, 168.90583282464658, 1741.277176069215, 164.18797001666712, 140.4174578131185, 201.62457583538108, 275.8138798398722, 122.48311749386284, 131.79861896042445, 518.844565320408, 1161.903842145896, 736.1485052181575, 721.2063022478392, 823.167117302031, 2973.634337511813, 835.0657075623632, 664.1248911770651, 770.0307464055813, 4904.519890267387, 758.985076593988, 2024.398276371174, 1452.3962021321374, 2009.9221754192322, 2992.4025920358877, 1422.3629879608568, 1671.6591066384422, 2468.8861555359836, 3968.271222892496, 2137.4619967707445, 2034.309914012574, 4191.09397050722, 8015.986993973521, 18512.098337835323, 3431.0131505325126, 9830.51345881912, 18481.91570180398, 8170.542817527109, 6378.06212334585, 8583.055198510468, 10148.143143503252, 8495.801510775338, 16652.127752289787, 14575.30561557727, 9764.246214484221, 14154.667023599612, 11145.310744371784, 30853.33204135951, 9808.898081801286, 39085.27693823087, 5699.7828442958635, 16884.622212718477, 6169.348374734068, 190312.42156538652, 59701.47857901389, 30226.910511956397, 27903.34880591638, 12809.45574577222, 15296.40452544682, 22032.996350660756, 70558.4993272785, 27732.41446942352, 35128.97281468118, 19471.924367421525, 61113.16816346781, 24791.030437809517, 97041.22763488715, 442231.5760819705, 29401.082918958713, 49396.448423884234, 17507.91174683316, 33462.19189871572, 87048.51232549419, 88379.01057504017, 33915.710737005626, 107586.06701835405, 53897.50793864486, 47030.66226291862, 29832.775672780597, 41283.58797336206, 80933.74040954813, 139526.0747711875, 358.3467923801952, 3444.3671439406785, 476.4062832329066, 3070.2893753626026, 291.51931150387975, 3202.265857192297, 136.3231902880723, 3099.336440191486, 318.3169230639345, 194.65833510540182, 158.3475304077303, 132.5449050275576, 3166.6319455978482, 103.66886684891337, 87.41833976720201, 84.39914668322969, 91.33435197392187, 130.373348431298, 78.641217657856, 105.6715007991242, 67.84351518345947, 65.60699211070057, 146.49045939131108, 121.55295061543491, 72.53482597022912, 59.72238372060977, 105.19462771832833, 63.30318445957859, 63.26798409756749, 66.42866801055447, 9510.811643842871, 3107.5426716479096, 3194.1867332679367, 3083.417370953766, 2689.15454165265, 3141.6353896193004, 3250.3673546338173, 3201.441904659979, 1103.1589428353711, 6764.490260301031, 3083.432537995367, 7135.535563912518, 3676.5483061254563, 3558.7875937908893, 3524.412071040772, 3602.7126040954886, 2527.595922806359, 3066.058678767962, 3337.8802301496185, 585.1557970160045, 3572.7471710710192, 3615.919810778879, 3776.60203291162, 3883.517793914495, 4016.764406812794, 7738.682349986661, 73210.74346957216, 12591.134046664003, 87048.51232549419, 15237.127682181486, 53897.50793864486, 97041.22763488715, 41283.58797336206, 36875.60149327188, 17837.924765337713, 20873.148580458994, 32952.658181936735, 47030.66226291862, 28231.12914816198, 9817.442995189142, 27916.63060874535, 442231.5760819705, 31367.42740670545, 15513.854483354375, 11452.71420795147, 59701.47857901389, 88379.01057504017, 43278.82146746735, 7109.546801460483, 26692.589242382768, 62162.6710533093, 29820.133716219665, 15645.284174926397, 39085.27693823087, 36936.49952123813, 54237.984815530814, 29832.775672780597, 24791.030437809517, 80933.74040954813, 49396.448423884234, 50048.512071083445, 2198.1426692799705, 2307.275369066101, 2197.0618618335443, 851.359901664134, 546.7248171580667, 583.8579420195963, 504.9207714603718, 478.3374560321217, 1204.230901093236, 490.01536467092853, 538.8655599305662, 552.6574972988643, 199.5924424192137, 190.40088735563612, 688.3873675263864, 137.31075771187818, 281.6111722989663, 546.6158509148773, 2425.6871288261973, 145.4536845928625, 125.29047928294014, 128.21047796718588, 259.1439376521763, 100.2791857574742, 103.76416465131095, 98.59488494578068, 142.48048713816462, 611.8038370643765, 87.49146626908966, 592.9888201816785, 33291.80409864335, 12449.061215251191, 9205.40644066361, 10224.89608977332, 28081.356141912624, 10095.72855854873, 3539.58112403971, 15612.025952806676, 13276.445208370702, 1346.4273593434875, 14690.664453235422, 2397.8809536529398, 555.2949087913803, 21777.9437472335, 16224.198189546421, 23857.266761060986, 22656.87822928485, 10486.342761854861, 14684.908223957562, 30804.632239970302, 29959.731123927457, 1522.5127294493411, 27848.74773479721, 29155.583356084175, 6905.295799764333, 12732.538405136833, 15128.564156497217, 35250.06660616206, 20390.58249991422, 33701.776328151944, 80933.74040954813, 8179.1078860905345, 28325.981564176, 7167.395976968701, 11281.171424952092, 17810.82767170141, 190312.42156538652, 54237.984815530814, 107586.06701835405, 36518.19530466961, 442231.5760819705, 42629.92685804523, 61113.16816346781, 34292.88602379202, 28146.309898806532, 70558.4993272785, 50048.512071083445, 34826.811719703495], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.1338, -8.5217, -8.0538, -9.1014, -9.4567, -9.4917, -8.5937, -9.1493, -9.8949, -8.4862, -9.9009, -9.2701, -9.0106, -9.6577, -9.6601, -9.9325, -9.8992, -9.9149, -9.9188, -6.8895, -8.9859, -10.1385, -10.0345, -9.0168, -10.3028, -9.2477, -10.0601, -10.3307, -10.3248, -10.0741, -7.6176, -7.4407, -7.3518, -7.2675, -8.117, -9.0157, -9.4883, -8.5662, -9.2498, -6.4432, -8.0172, -6.3386, -6.502, -7.6153, -6.7786, -7.3171, -7.407, -6.0051, -8.3248, -8.0814, -3.4918, -6.5318, -5.8051, -6.1494, -7.5001, -7.1218, -6.1982, -4.6783, -5.0688, -6.0459, -6.7647, -6.3928, -7.0248, -7.3494, -6.1352, -6.6735, -6.914, -6.8007, -5.3052, -5.6345, -5.8208, -5.5858, -5.7688, -5.6521, -5.0347, -6.2632, -5.7729, -5.654, -3.0871, -5.2904, -4.7351, -5.964, -5.2093, -4.5805, -4.5087, -6.1137, -5.1217, -5.2166, -5.3965, -5.4687, -5.5627, -5.9133, -5.7582, -5.7284, -5.6268, -5.7473, -5.7078, -5.6937, -5.7131, -5.7476, -7.8919, -7.4165, -8.0367, -8.4524, -7.4782, -8.5057, -8.4199, -8.5509, -8.6208, -8.6147, -8.683, -8.6968, -8.7078, -8.5683, -8.9588, -8.334, -9.1345, -9.1525, -9.7663, -8.829, -7.8521, -9.8802, -7.5478, -9.9095, -10.0661, -9.7048, -9.3916, -10.2035, -10.1303, -8.7603, -7.9582, -8.4114, -8.4325, -8.3017, -7.0305, -8.2884, -8.516, -8.3719, -6.5414, -8.3893, -7.4329, -7.7614, -7.4504, -7.0685, -7.7843, -7.6312, -7.2624, -6.8205, -7.4077, -7.4557, -6.7767, -6.1738, -5.3972, -6.9682, -5.9903, -5.4056, -6.169, -6.4011, -6.1279, -5.9766, -6.1427, -5.5312, -5.6553, -6.0257, -5.6896, -5.9069, -4.9957, -6.0354, -4.8283, -6.527, -5.5819, -6.4687, -3.5419, -4.5288, -5.1194, -5.2083, -5.8683, -5.7262, -5.438, -4.5071, -5.2575, -5.1103, -5.5944, -4.7538, -5.4168, -4.4671, -3.4762, -5.3383, -5.0092, -5.6801, -5.2993, -4.7972, -4.7994, -5.3824, -4.8836, -5.2198, -5.2867, -5.4867, -5.403, -5.576, -5.5713, -8.6619, -6.3989, -8.3782, -6.5155, -8.8701, -6.474, -9.6326, -6.509, -8.7856, -9.2775, -9.4844, -9.6625, -6.4895, -9.909, -10.081, -10.1162, -10.0374, -9.6816, -10.1879, -9.8939, -10.3373, -10.3711, -9.5681, -9.7549, -10.2712, -10.4657, -9.8997, -10.4078, -10.4086, -10.36, -5.4002, -6.5181, -6.4925, -6.5303, -6.6659, -6.5126, -6.4792, -6.4978, -7.553, -5.7626, -6.5404, -5.724, -6.3804, -6.4124, -6.4261, -6.4046, -6.7554, -6.5662, -6.4858, -8.2001, -6.4295, -6.4266, -6.3876, -6.3637, -6.3402, -5.7315, -3.6474, -5.3365, -3.7104, -5.2085, -4.1606, -3.7204, -4.4898, -4.5996, -5.1934, -5.0948, -4.8118, -4.558, -4.926, -5.687, -4.9363, -2.9837, -4.8882, -5.5126, -5.7265, -4.7596, -4.5498, -4.9821, -6.0206, -5.2952, -4.8673, -5.3105, -5.6354, -5.2842, -5.3086, -5.2285, -5.4151, -5.488, -5.2554, -5.3746, -5.4133, -6.7974, -6.749, -6.7981, -7.7466, -8.19, -8.1243, -8.2696, -8.3238, -7.4006, -8.2999, -8.2052, -8.1811, -9.2001, -9.2475, -7.9635, -9.576, -8.8579, -8.195, -6.705, -9.5191, -9.6685, -9.6455, -8.9429, -9.8924, -9.8586, -9.9098, -9.5421, -8.085, -10.03, -8.1164, -4.1024, -5.082, -5.3838, -5.2807, -4.2819, -5.2956, -6.3334, -4.8681, -5.0301, -7.2988, -4.9375, -6.7338, -8.1834, -4.5552, -4.8453, -4.4755, -4.5266, -5.284, -4.9548, -4.2327, -4.2623, -7.1865, -4.3457, -4.3013, -5.7099, -5.14, -4.9817, -4.1788, -4.7016, -4.3589, -3.5807, -5.6404, -4.5652, -5.7655, -5.3836, -5.0215, -3.2019, -4.2542, -3.8808, -4.6247, -3.3928, -4.636, -4.4803, -4.7921, -4.8663, -4.5755, -4.7491, -4.8547], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0852, 1.0852, 1.085, 1.0834, 1.0834, 1.0832, 1.0825, 1.0824, 1.0818, 1.0818, 1.0817, 1.0817, 1.0816, 1.0815, 1.0815, 1.0814, 1.0811, 1.081, 1.0808, 1.0807, 1.0803, 1.0799, 1.0799, 1.0797, 1.0792, 1.0791, 1.079, 1.0788, 1.0787, 1.0785, 1.0776, 1.071, 1.0668, 1.066, 1.0733, 1.0771, 1.0779, 1.0685, 1.0747, 1.0451, 1.0612, 1.0261, 1.0245, 1.045, 1.0242, 1.0336, 1.0297, 0.9831, 1.0517, 1.0436, 0.8795, 0.986, 0.945, 0.9516, 1.0096, 0.9882, 0.9394, 0.8438, 0.8686, 0.9303, 0.9642, 0.9402, 0.9795, 0.999, 0.9018, 0.9452, 0.9654, 0.9529, 0.7815, 0.8118, 0.8346, 0.8004, 0.8126, 0.7889, 0.6939, 0.8596, 0.7639, 0.7149, 0.1306, 0.6209, 0.4447, 0.7573, 0.491, 0.2474, 0.1225, 0.7896, 0.2748, 0.3253, 0.3626, 0.3457, 0.4084, 0.6476, 0.3574, 0.185, -0.2172, 0.0384, -0.1509, -0.3776, -0.66, -1.6868, 1.212, 1.212, 1.2119, 1.2116, 1.2116, 1.2114, 1.2114, 1.2113, 1.2112, 1.2111, 1.2109, 1.2109, 1.2108, 1.2107, 1.2107, 1.2106, 1.2099, 1.2096, 1.2084, 1.2081, 1.2079, 1.2077, 1.2071, 1.2067, 1.2066, 1.2061, 1.2059, 1.2058, 1.2057, 1.2054, 1.2012, 1.2045, 1.2039, 1.2024, 1.1892, 1.2013, 1.2028, 1.1989, 1.1779, 1.196, 1.1713, 1.1749, 1.1611, 1.145, 1.1729, 1.1645, 1.1433, 1.1107, 1.1422, 1.1437, 1.0998, 1.0543, 0.9939, 1.1085, 1.0337, 0.9871, 1.04, 1.0556, 1.0318, 1.0156, 1.0273, 0.9657, 0.9749, 1.0051, 0.9699, 0.9916, 0.8846, 0.9908, 0.8155, 1.0421, 0.9012, 1.0212, 0.519, 0.6913, 0.7814, 0.7725, 0.891, 0.8557, 0.7789, 0.546, 0.7294, 0.6402, 0.7462, 0.4429, 0.6823, 0.2673, -0.2586, 0.5902, 0.4005, 0.7668, 0.4998, 0.0458, 0.0284, 0.4033, -0.2523, 0.1026, 0.172, 0.4273, 0.186, -0.6601, -1.2, 1.6739, 1.6738, 1.6728, 1.6723, 1.6721, 1.6716, 1.6697, 1.6693, 1.6686, 1.6686, 1.6681, 1.6679, 1.6674, 1.667, 1.6655, 1.6655, 1.6653, 1.6652, 1.6644, 1.663, 1.6628, 1.6624, 1.6622, 1.6621, 1.662, 1.6619, 1.6618, 1.6615, 1.6613, 1.6611, 1.6569, 1.6576, 1.6557, 1.6531, 1.6544, 1.6522, 1.6516, 1.6481, 1.6583, 1.6352, 1.6431, 1.6204, 1.6272, 1.6277, 1.6237, 1.6232, 1.6268, 1.6229, 1.6184, 1.6453, 1.6067, 1.5976, 1.593, 1.5891, 1.5788, 1.5317, 1.3688, 1.44, 1.1326, 1.3772, 1.1618, 1.0139, 1.0992, 1.1023, 1.2348, 1.1762, 1.0026, 0.9007, 1.0431, 1.3383, 1.044, 0.2339, 0.9755, 1.0551, 1.1448, 0.4606, 0.2781, 0.5597, 1.3275, 0.7299, 0.3124, 0.6038, 0.924, 0.3596, 0.3917, 0.0876, 0.4988, 0.611, -0.3395, 0.0351, -0.0167, 1.7245, 1.7244, 1.7243, 1.7238, 1.7233, 1.7233, 1.7233, 1.7232, 1.7231, 1.7229, 1.7226, 1.7214, 1.7209, 1.7207, 1.7194, 1.7191, 1.7189, 1.7185, 1.7184, 1.7183, 1.7182, 1.7181, 1.717, 1.7169, 1.7165, 1.7164, 1.716, 1.7158, 1.7158, 1.7157, 1.7018, 1.7058, 1.706, 1.704, 1.6925, 1.7018, 1.7121, 1.6933, 1.6934, 1.7133, 1.6848, 1.7011, 1.7144, 1.6734, 1.6777, 1.6619, 1.6625, 1.6755, 1.6679, 1.6491, 1.6473, 1.7027, 1.637, 1.6356, 1.6674, 1.6254, 1.6112, 1.5683, 1.5928, 1.4331, 1.3352, 1.5676, 1.4005, 1.5744, 1.5028, 1.4083, 0.859, 1.062, 0.7505, 1.087, -0.1751, 0.921, 0.7165, 0.9825, 1.1057, 0.4776, 0.6474, 0.9045]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 4, 1, 2, 3, 4, 4, 2, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 2, 1, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 2, 3, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 3, 1, 3, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 3, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 3, 4, 2, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 4, 1, 2, 3, 4, 3, 4, 3, 3, 3, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 2, 1, 2, 4, 1, 4, 1, 2, 3, 4, 2, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 2, 2, 3, 4, 2, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 2, 4, 1, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 4, 2, 2, 1, 2, 1, 2, 3, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 1, 2, 3, 4, 3, 1, 3, 1, 2, 3, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 2, 2, 1, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 1, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 3, 1, 2, 3, 4, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 2, 3, 4, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 4, 1, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 4, 1, 2, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 3, 1, 2, 3, 4, 2, 4], \"Freq\": [0.7605064183324252, 0.07416865310253502, 0.12977880622170446, 0.035483329193547596, 0.006826383125257057, 0.9829991700370161, 0.7426846021867797, 0.0451970140168713, 0.1872042150924933, 0.0249264939113147, 0.9900058592745073, 0.38280287862729157, 0.13898988840352827, 0.38804778007648133, 0.0901748413442837, 0.9981764040768046, 0.9988182448419394, 0.9655595479246339, 0.001069279676549982, 0.026731991913749555, 0.006415678059299893, 0.9784932010028038, 0.017204851512011884, 0.9329945191371016, 0.03391813583796629, 0.015730149953839437, 0.002690310607152188, 0.9765827503962442, 0.008070931821456564, 0.01244268655807887, 0.9872240812948417, 0.008226867344123681, 0.027908163413532805, 0.8544915202458078, 0.08670345150384068, 0.030887124676775072, 0.017591687936412893, 0.0005622913952278094, 0.000722946079578612, 0.9811181573303519, 0.9985305637590051, 0.0009368366443598031, 0.001249115525813071, 0.9949205163101109, 0.0028105099330794094, 0.9953313169467288, 0.002835701757682988, 0.007351607569813797, 0.0040842264276743315, 0.027840810148646696, 0.9606781262294641, 0.02754301635969422, 0.8306455831212056, 0.11123141222184203, 0.030485646312653002, 0.9905586658253026, 0.9990134357097672, 0.03385589279297031, 0.07856634663406852, 0.8651343597287644, 0.0223552269205491, 0.0017213128379937547, 0.9888942254274121, 0.0017213128379937547, 0.008606564189968774, 0.9960556401261828, 0.00029032909623447, 0.9972804455654046, 0.00232263276987576, 0.23336105366824864, 0.15947297422686396, 0.4711323819744346, 0.13601542651493648, 0.9935552544943658, 0.9986446687122056, 0.9979278921761762, 0.9929238577814952, 0.0021967341986316264, 0.004393468397263253, 0.0021967341986316264, 0.021616770913106327, 0.04973293640307685, 0.0127474313524132, 0.915911919735221, 0.99567162122592, 0.003908426383614995, 0.0016863724339585733, 0.0033727448679171466, 0.00505911730187572, 0.9915869911676412, 0.9925901405211137, 0.0019273594961575025, 0.005782078488472508, 0.018523599081674325, 0.6720945086692028, 0.26792697456256015, 0.04144783219932653, 0.17439429859363215, 0.5133187404114394, 0.025000531712244378, 0.287279352498409, 0.0014874563503300018, 0.0022311845254950028, 0.9780025503419763, 0.018221340291542522, 0.9911348285178503, 0.001623037382944078, 0.0010820249219627186, 0.005951137070794953, 0.03518560617580673, 0.8344348060633371, 0.09157578296087447, 0.03891388232688559, 0.0012494370096729382, 0.007808981310455864, 0.9720619935255459, 0.018741555145094072, 0.9909405889509901, 0.014292467817950369, 0.7810413295809349, 0.006485657665288403, 0.19811282072024483, 0.005488315047905883, 0.001829438349301961, 0.9933850236709648, 0.017223266079107014, 0.9817261665090997, 0.2332099669050517, 0.0491005749943299, 0.17927749741055618, 0.5384009504081586, 0.07669996111175392, 0.8360702657526597, 0.018818955975695592, 0.0684603101169899, 0.05166702083589011, 0.3581568542332261, 0.5613853140612233, 0.0288007912676387, 0.9963156257073694, 0.009004224459544718, 0.0018008448919089436, 0.9886638456580101, 0.03685872667557016, 0.93280931355866, 0.018631883814024475, 0.011746187621884993, 0.035423041098184144, 0.9343662538728571, 0.010359568623053853, 0.019716598347102494, 0.8683943727569733, 0.05014931409551948, 0.017515385437773347, 0.0639772499674458, 0.19163668791324712, 0.564035848828443, 0.10299190981433869, 0.14133632731569695, 0.737196094330739, 0.03769050993011291, 0.2202862890101419, 0.0048607211537777745, 0.8043218490004067, 0.059027758684161775, 0.1200116698363452, 0.016610136746008315, 0.006914397150667224, 0.803366518943148, 0.12121802504763476, 0.06849574677379718, 0.053522478398579224, 0.39640639031005964, 0.5307262037365404, 0.01934035288261036, 0.993966371114406, 0.06982111278308342, 0.0033771741841086473, 0.02151966805687836, 0.9053182981446599, 0.4849424671981943, 0.03879195175467822, 0.036035454812821, 0.4402355324224474, 0.0003183055561775992, 0.00413797223030879, 0.9759248352405192, 0.01973494448301115, 0.033216964843008234, 0.9319463939052451, 0.024795762488442768, 0.010292580655580016, 0.10185596567652731, 0.2905444132743114, 0.5631365770071262, 0.04447385082787667, 0.831426877933891, 0.08780065691617193, 0.03555308290619638, 0.04524000984295714, 0.021882375868556656, 0.8256949827735378, 0.03711899313999611, 0.1152471795743984, 0.9870470488040979, 0.007101057905065452, 0.001775264476266363, 0.004438161190665908, 0.993039396876616, 0.8849317788799004, 0.044256881241958264, 0.05125564385696562, 0.019349520170902683, 0.05282196785189716, 0.010785637414785283, 0.9239696051999393, 0.012168411442321858, 0.006283046407803762, 0.003141523203901881, 0.9927213324329943, 0.4768052371754537, 0.012563478118039578, 0.5096705676146732, 0.0009407435305778427, 0.997736829094409, 0.009463289462510292, 0.9841821041010704, 0.05527166915219458, 0.7248551526527213, 0.18392662785673788, 0.035910971483064276, 0.014176355423727112, 0.00511581521812761, 0.026873438977152265, 0.9538838110330469, 0.9941832210391784, 0.06697153527830634, 0.3884740632284339, 0.5155025468991036, 0.02905981373824032, 0.9993198728939635, 0.11375617086982838, 0.46305895849327866, 0.05838348930718464, 0.3647986296564951, 0.08569933937647738, 0.31129768075385844, 0.5804464504926639, 0.022562131707158387, 0.039457629536084705, 0.6437936939021123, 0.2755941608832801, 0.04114201517477315, 0.07389788811393716, 0.7842642982340501, 0.10893933410295516, 0.032851355614704376, 0.8554603017020778, 0.06749040693546204, 0.015902911979045652, 0.06118742352913297, 0.9591684312725437, 0.028733259106748312, 0.006791497607049601, 0.005224228928499693, 0.005987552781439773, 0.9891437194938506, 0.004790042225151819, 0.9883442896033129, 0.006315223214565407, 0.9914900446867689, 0.2187123952805229, 0.6495536483040064, 0.09991097167556785, 0.031825945282084864, 0.007670279332642618, 0.9894660339108977, 0.06148043888918716, 0.12536431192413888, 0.06622796312773443, 0.7469042508294494, 0.9837752607635983, 0.0456768490063684, 0.8124539084473554, 0.03492333074253728, 0.10702311034003358, 0.0016345108340580347, 0.008172554170290175, 0.9905135654391691, 0.7431954260996974, 0.16046674653778034, 0.07040703878989127, 0.02596315899038244, 0.0036386565792459725, 0.05094119210944362, 0.9326156709267369, 0.012875246357331902, 0.1613168435858251, 0.24872532512138, 0.061558354164137644, 0.5283941289818505, 0.0008690545117770256, 0.01748972204951264, 0.0003258954419163846, 0.981271175610234, 0.18080645502294465, 0.1810939056668603, 0.6063292248994138, 0.03171538771202686, 0.062465707189352244, 0.4996415852305793, 0.01722956375116807, 0.4206661832238528, 0.05383596911722955, 0.7322639407760464, 0.0777630665026649, 0.13615939824038584, 0.004819621525947193, 0.9625472704677394, 0.002065552082548797, 0.030983281238231956, 0.004216101304501254, 0.004864732274424524, 0.9768382407044445, 0.014269881338311937, 0.010767745605858841, 0.9529454861185074, 0.00358924853528628, 0.0329014449067909, 0.024923546978856334, 0.0040177302506899555, 0.026421683682503434, 0.9446432887723908, 0.020145203957105232, 0.010498204879054838, 0.9485269975859547, 0.020996409758109676, 0.507605123222637, 0.24418425729071055, 0.20294267197504115, 0.0452521697740779, 0.9853904697949479, 0.06184227449143764, 0.03766444631992396, 0.024226870419326085, 0.8762378416641684, 0.9955899651735403, 0.0014428840074978844, 0.0014428840074978844, 0.987904305293821, 0.35063396112246503, 0.4450444844586686, 0.09505919026730804, 0.1092414081700919, 0.09001526517549024, 0.7882510530497008, 0.09536678246488677, 0.026345931270875193, 0.008210028986620327, 0.0026362478397404716, 0.020110804948877313, 0.9690093845217477, 0.00293401514661897, 0.015843681791742437, 0.00195601009774598, 0.9792764554365249, 0.8987392927989484, 0.012200080897723281, 0.03687135560200814, 0.052324791405790966, 0.9941714842245777, 0.004177191110187301, 0.9918462903175566, 0.999480166007448, 0.99814713150802, 0.23131390376449804, 0.049190618144721886, 0.20413737784796127, 0.5153399429397006, 0.9909047764606937, 0.007027693450075842, 0.004736894043165492, 0.0006315858724220656, 0.990958233830221, 0.0037895152345323938, 0.9982414536702862, 0.002399618879015111, 0.4321614346154117, 0.3059323681502723, 0.24697040460152356, 0.014946987881001173, 0.4060147269538332, 0.38721836931881914, 0.12031679192038414, 0.086409636970947, 0.046755408485095164, 0.32947719995171426, 0.5976714180676074, 0.02608654933731897, 0.9897001009946982, 0.27155798499703565, 0.4437970886465644, 0.1936981363092021, 0.09095795635840773, 0.9994472402023615, 0.07934476855092727, 0.5936201387892891, 0.29642481930457254, 0.030602257154854155, 0.31558888252130857, 0.1276278569019998, 0.5371972522329628, 0.01953092961682118, 0.8750437723469833, 0.02743083925852612, 0.08334755005475245, 0.014137432540932693, 0.8860642389066691, 0.0337388759616096, 0.06364922699140534, 0.01651051376844725, 0.9163602417582575, 0.011091171540479745, 0.07037157115338873, 0.002294725146306154, 0.9914807906658408, 0.005137206169253061, 0.9983937367779123, 0.032346812064215816, 0.2933076615597524, 0.6428998973178977, 0.031449846738345014, 0.9947882989430719, 0.003430304479114041, 0.979413603427633, 0.996241650800752, 0.9799585190574099, 0.997037743806016, 0.0028650509879483217, 0.030068097243810286, 0.007121391452481384, 0.9514970246232072, 0.011077720037193263, 0.001198365346925795, 0.027562402979293285, 0.9434131193673321, 0.028161585652756182, 0.04865830657268399, 0.006371921098803856, 0.0010137147202642497, 0.9440580373432349, 0.1318246455594269, 0.333116256699021, 0.49608786204358946, 0.03898949008928153, 0.13708078282969757, 0.3531738487360442, 0.46029545318710924, 0.04945709645755802, 0.9916166193937997, 0.010728560321349754, 0.8534195483529495, 0.0794661967988348, 0.05651206773920277, 0.10589620029909427, 0.013393985808581093, 0.020370020083883746, 0.8602845468303232, 0.01144322913657223, 0.9497880183354952, 0.00995063403180194, 0.02935437039381572, 0.020693440862079294, 0.8210369012516413, 0.02936497798523633, 0.1288905745123796, 0.9834222650558184, 0.013570095432487923, 0.008221298692675166, 0.0010895697062581545, 0.9771459229306084, 0.9939406120737558, 0.004159697427282137, 0.9913945535022427, 0.004159697427282137, 0.007018504920117875, 0.9896091937366204, 0.9943987603697171, 0.0013101432942947524, 0.003144343906307406, 0.0010481146354358018, 0.9992052381041334, 0.99929452308644, 0.9984827029061576, 0.0007791494322802858, 0.994973825021925, 0.0007791494322802858, 0.003895747161401429, 0.02977460135722658, 0.3420149956488461, 0.5875463124128377, 0.04060172912349079, 0.9938281980948084, 0.995749956974415, 0.0020701662307160396, 0.0020701662307160396, 0.9973967960523588, 0.9961640693293556, 0.0012986494431136545, 0.9856749273232639, 0.0012986494431136545, 0.011687844988022892, 0.02753746814532657, 0.00505790231240692, 0.9522906075970585, 0.015173706937220761, 0.19215780890176404, 0.6166430268397572, 0.15609892188698377, 0.035085297065381195, 0.11963874354842109, 0.6479826789229165, 0.19053241480131705, 0.0418917148312567, 0.18676182830188326, 0.5365108504159364, 0.2160124515653361, 0.06071204944798063, 0.7195366494738555, 0.136406832963761, 0.11536335460020558, 0.028709316910279174, 0.20033444708367093, 0.20331900781190104, 0.3420507800892942, 0.2542912809232469, 0.009506188877607173, 0.988643643271146, 0.9986821200978201, 0.0010761660776916165, 0.7243847851524744, 0.0896184782942497, 0.14161469443149918, 0.04437177098250857, 0.05243343770944289, 0.6401677231453551, 0.18882891619544465, 0.1185749637089362, 0.9966018881497976, 0.9972365352942479, 0.3208828791010925, 0.11331796330305756, 0.08981454631328288, 0.47598793489341445, 0.29019519646830716, 0.2573309599270333, 0.004902659127142203, 0.44757290021948914, 0.9978945089951848, 0.0012905988353277545, 0.0003226497088319386, 0.9927931540758751, 0.005485045050142956, 0.20157998380736947, 0.0409475257658224, 0.7130166178128279, 0.04441074933805613, 0.10280319587295515, 0.7979692277549172, 0.06189834530455826, 0.0373878991306379, 0.001252274940077672, 0.001878412410116508, 0.9792790031407395, 0.01721878042606799, 0.23011312052463367, 0.04093087901752347, 0.7053895473294852, 0.023630198195683655, 0.997027023422728, 0.0018094389470649365, 0.99700085983278, 0.9928951031708965, 0.005531449042734799, 0.015998191689277, 0.004922520519777538, 0.9752743779809248, 0.00399954792231925, 0.013175489622109404, 0.9828915258093616, 0.0013175489622109405, 0.0013175489622109405, 0.2150486740910772, 0.49016514069508726, 0.18083095149042636, 0.11394949893125032, 0.009195325417780571, 0.004597662708890286, 0.007881707500954776, 0.977988539076805, 0.01857413546277092, 0.012228523347375873, 0.07667614639435683, 0.8925500041060347, 0.02518121790441128, 0.8009054569111539, 0.026302648661287894, 0.14762106690521268, 0.9045638318572378, 0.02233490942857377, 0.029335403428574505, 0.04383642671429031, 0.9959537064149594, 0.9952378569624544, 0.0021873359493680316, 0.0021873359493680316, 0.15831689216685976, 0.026947556113508044, 0.013916994440199879, 0.8008920048866618, 0.9019589829829326, 0.05780781729890613, 0.010796026999795712, 0.029541855699440994, 0.9926355630204431, 0.9982106204210162, 0.9988083236502012, 0.9848300876952281, 0.014860261954248983, 0.9487537924476811, 0.02338342680376103, 0.02108341760994847, 0.0069000275814376805, 0.9964434905291103, 0.6753732451897412, 0.13404971046633776, 0.1701732247972831, 0.020415216657350004, 0.021204929804929402, 0.9656398803475543, 0.0050973388954157215, 0.008155742232665155, 0.9964677825613515, 0.81311683272142, 0.08956032068194078, 0.03251721950495881, 0.06480509119767194, 0.9958496575127206, 0.38451347483265474, 0.22960594740792342, 0.23630831820256476, 0.14957321814519053, 0.9902937256289542, 0.9903425123619075, 0.9935480451436961, 0.02563419874927726, 0.005126839749855451, 0.9689727127226804, 0.9989560057639305, 0.9934235367671661, 0.0036256333458655698, 0.004954804675511519, 0.900900073647418, 0.050130964952234194, 0.044010323882484666, 0.6278027374030763, 0.2188065880133732, 0.06686221176879241, 0.08648627092293298, 0.9875667529729485, 0.7845696584343391, 0.08295588407884015, 0.08295588407884015, 0.04951981501811703, 0.0036444604466619626, 0.9900784213431665, 0.0012148201488873208, 0.006074100744436604, 0.9927645733329516, 0.9985065969788595, 0.00045515331970009086, 0.9995166900613995, 0.9400797526832956, 0.05195575810751893, 0.005716814807947067, 0.002353982567978204, 0.9986742559779263, 0.6448257611728418, 0.19767913734753656, 0.1353922848089507, 0.022149281581038745, 0.019372283776140597, 0.004344519523325648, 0.008083655178646903, 0.9681512482020853, 0.025359149732418648, 0.010688567242589679, 0.02498190618268019, 0.939001111248996, 0.9987992916830478, 0.0006721394964219702, 0.0003360697482109851, 0.5263930819822454, 0.16934600495162577, 0.2556035596728775, 0.04866264510104189, 0.873896881904738, 0.022303377800686676, 0.08690626522336532, 0.01680993499263577, 0.8633693229397453, 0.07430263972501504, 0.03532794220054396, 0.02712274272170794, 0.990756500622451, 0.0196006394430883, 0.002085174408839181, 0.002085174408839181, 0.9766956931002724, 0.03889517778448587, 0.005711879255064358, 0.9517078815938186, 0.003807919503376239, 0.04921870306877413, 0.014474071564475737, 0.02174540609449672, 0.9145418108890545, 0.033475114052514285, 0.014042526657622517, 0.097474993122406, 0.8550338453752377, 0.9397707838248752, 0.034576472235066165, 0.01573672774801088, 0.009973982375499854, 0.045419653000090904, 0.5882772818413196, 0.34451976578488136, 0.021782071598622638, 0.20742685333955233, 0.6270566673126848, 0.11236691139067603, 0.05315345214321284, 0.0015992569544347583, 0.9979363395672891, 0.14349988908025013, 0.4558409230559032, 0.30794989044154586, 0.09268329673134586, 0.046998101420661936, 0.8413149717854952, 0.04198007496689334, 0.06964041590717876, 0.7971447862742744, 0.09297589562404553, 0.026051217131375076, 0.08376813784485261, 0.04829757993049295, 0.028132095526524244, 0.906948884983432, 0.016431135440270795, 0.03654078422809277, 0.00926759020277715, 0.9198745246985092, 0.03442247789602942, 0.020342381364595177, 0.02677984382174555, 0.916179656901641, 0.03682228525490013, 0.0038588593237407804, 0.9917268462013805, 0.04210686732393381, 0.8430145728812581, 0.025615010955393067, 0.08930164778284296, 0.9918533973832817, 0.9951484428961268, 0.03192039239246294, 0.003608392183495811, 0.9478968697413996, 0.01637654914048099, 0.03585649180817593, 0.15366891406556998, 0.13318302039983748, 0.6772947811705623, 0.0006675349222288644, 0.04605990963379165, 0.0015257941079516901, 0.9518094369666137, 0.03477972525718397, 0.014079609590154427, 0.011652090695300215, 0.939493949015759, 0.777384789229587, 0.04362191775928536, 0.1616825274368351, 0.017378409171844328, 0.9788401509247562, 0.0006308609848124047, 0.006203466350655313, 0.9804631138959458, 0.012617219696248095, 0.13421527407495706, 0.11506507925986548, 0.7350833695927019, 0.01563978107223955, 0.003913819420058188, 0.0016307580916909118, 0.9477966028907578, 0.04663968142236007, 0.0008304055302784298, 0.9981474473946725, 0.9796649305664812, 0.004521530448768375, 0.0011303826121920938, 0.014318179754433186, 0.9916590386343987, 0.006316299609136298, 0.05921974250255685, 0.9029120739857923, 0.01562393206450436, 0.022175903575425544, 0.9946370542124084, 0.020996904535965335, 0.8930842463422528, 0.029347946112769727, 0.056548481534361185, 0.007733609558956492, 0.9610767515539568, 0.011248886631209444, 0.020388607019067116, 0.5513516511842256, 0.06134853138146109, 0.27669108151744587, 0.11059521213295169, 0.9878325005720066, 0.0027593086608156607, 0.008277925982446982, 0.1813990180860833, 0.0517814129051808, 0.7413470724675821, 0.025464116865919074, 0.9064253829300578, 0.03555791063355945, 0.02621642563660739, 0.03164051369935375, 0.013683569186271572, 0.01515294574318664, 0.021305960075268488, 0.9498601080107844, 0.03144154838406255, 0.22682831334216555, 0.013194221196883392, 0.7286017381785352, 0.00646937177181944, 0.0012170105313323697, 0.02337941283875342, 0.9689325424981452, 0.07884205716406888, 0.6997069136210855, 0.19782426615130383, 0.023665692117241236, 0.006488083354089004, 0.0026733306412681547, 0.013667027435696746, 0.9771474055179139, 0.9958382742187845, 0.002514743116714102, 0.002514743116714102, 0.9940979091608982, 0.00516861997830623, 0.0005742911087006922, 0.9990381876895612, 0.0005060983726897473, 0.9754408743367264, 0.005555806444554178, 0.016667419333662534, 0.0031747465397452446, 0.005913231959952579, 0.013748264306889746, 0.9595697163013047, 0.020696311859834025, 0.7598467910281222, 0.08796663699003353, 0.1293526277867722, 0.022833650094752377, 0.9914564660333315, 0.00218864562038263, 0.00218864562038263, 0.0065659368611478905, 0.05634656712619078, 0.8015030002201627, 0.04800225065686635, 0.09412030082065943, 0.021205397306586625, 0.17528206687504375, 0.78936495816541, 0.014136931537724417, 0.46715689832723034, 0.1921494097580991, 0.32731944909009525, 0.013378367995423206, 0.037418354601206696, 0.010791061064767474, 0.9454090641937842, 0.006306464258630342, 0.03555531695991375, 0.7201815340467488, 0.09029818874231513, 0.1539541983223248, 0.0012651204673128733, 0.9981800487098571, 0.9449773259779033, 0.014203903219827016, 0.02214137854855388, 0.018799283673300464, 0.9943826947924487, 0.9262054550834926, 0.03640108775437307, 0.017975845804628677, 0.019773430385091544, 0.957947908459133, 0.0024946560116123257, 0.03159897614708946, 0.008315520038707752, 0.022074602115121876, 0.03402085737742313, 0.016880578088034375, 0.927035900884611, 0.4823932468763878, 0.2990657674054259, 0.0613962498362483, 0.15712026260699213, 0.991942570350613, 0.004959712851753065, 0.013337296477251862, 0.9592973984749302, 0.015807166195261464, 0.01136140070284418, 0.9414627942811161, 0.036182604732359465, 0.01658965276266284, 0.005577555670205609, 0.012470797722751907, 0.01980656108907656, 0.11333754400971587, 0.8543719067312777, 0.9970481429771171, 0.0020990487220570887, 0.9981628148109848, 0.0008430429179146831, 0.9970317392180142, 0.0784836834611994, 0.34914671962405774, 0.5312245667410256, 0.041158262116347835, 0.8640003176358367, 0.02216806459617285, 0.09130471605548691, 0.02258371580735109, 0.9916477379569827, 0.008150529353071092, 0.9595177503268841, 0.005502378733532477, 0.02718822433039577, 0.007606229425765484, 0.005650386104775606, 0.002542673747149023, 0.004520308883820485, 0.9874049718095372, 0.9907775009513502, 0.0015057408829047875, 0.0075287044145239375, 0.44414906817665933, 0.031489447633560444, 0.1839215516922106, 0.3404496816169013, 0.9984026712304915, 0.6897511294609996, 0.158555583264072, 0.08680667221022002, 0.06488045859656127, 0.9829171177277113, 0.0013843903066587482, 0.00830634183995249, 0.00830634183995249, 0.9930621945599395, 0.004262069504549096, 0.002131034752274548, 0.002252583709908622, 0.0003217976728440889, 0.9811611045016271, 0.016089883642204445, 0.9872437560415787, 0.003550995480173535, 0.003550995480173535, 0.9942787344485898, 0.9957596663044593, 0.0012948760290044984, 0.0038846280870134956, 0.38142485488385136, 0.2310336337116924, 0.010122128544899944, 0.377409465047858, 0.014719758270720714, 0.0014018817400686394, 0.05854525171619985, 0.9253754609919753, 0.004358011406833388, 0.001452670468944463, 0.995079271226957, 0.03177295014334946, 0.02820731907170691, 0.21704455275701387, 0.7229758288729484, 0.009655181105602866, 0.000742706238892528, 0.000742706238892528, 0.9885420039659549, 0.006810591683530958, 0.0025945111175356026, 0.967104019061396, 0.023350600057820425, 0.9806331852463955, 0.005333988036905081, 0.005333988036905081, 0.008616442213462054, 0.8681660354175201, 0.06984621614237532, 0.04803377654973984, 0.014000182859102917, 0.9907480576204991, 0.7513053852519107, 0.0558533411045174, 0.11977020594885926, 0.07310928190774174, 0.006183814813437452, 0.9935329133589507], \"Term\": [\"accord\", \"accord\", \"accord\", \"accord\", \"adjunct\", \"adjunct\", \"adults\", \"adults\", \"adults\", \"adults\", \"agar\", \"age\", \"age\", \"age\", \"age\", \"ager\", \"alak\", \"anderson\", \"anderson\", \"anderson\", \"anderson\", \"anecdotal\", \"anemia\", \"anemia\", \"anemia\", \"anemia\", \"ang\", \"ang\", \"ang\", \"ang\", \"antiviral\", \"antiviral\", \"anxiety\", \"anxiety\", \"anxiety\", \"anxiety\", \"api\", \"api\", \"api\", \"api\", \"apko\", \"approval\", \"approval\", \"approval\", \"approval\", \"argentine\", \"argentine\", \"arm\", \"arm\", \"arm\", \"arm\", \"arthritis\", \"arthritis\", \"arthritis\", \"arthritis\", \"atin\", \"ating\", \"attack\", \"attack\", \"attack\", \"attack\", \"ay\", \"ay\", \"ay\", \"ay\", \"ayaw\", \"baba\", \"baba\", \"baba\", \"back\", \"back\", \"back\", \"back\", \"baptists\", \"bato\", \"bdi\", \"behead\", \"behead\", \"behead\", \"behead\", \"belly\", \"belly\", \"belly\", \"belly\", \"betja\", \"betja\", \"bhi\", \"bhi\", \"bhi\", \"bhi\", \"binder\", \"binder\", \"binder\", \"blood\", \"blood\", \"blood\", \"blood\", \"body\", \"body\", \"body\", \"body\", \"boom\", \"boom\", \"boom\", \"boom\", \"boris\", \"boris\", \"boris\", \"boris\", \"brain\", \"brain\", \"brain\", \"brain\", \"breakthrough\", \"breakthrough\", \"breakthrough\", \"breakthrough\", \"britons\", \"burn\", \"burn\", \"burn\", \"burn\", \"c_\", \"c_\", \"c_\", \"caesar\", \"caesar\", \"call\", \"call\", \"call\", \"call\", \"calories\", \"calories\", \"calories\", \"calories\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cancercarepune\", \"cappuccino\", \"cappuccino\", \"cappuccino\", \"carb\", \"carb\", \"carb\", \"carb\", \"carbohydrates\", \"carbohydrates\", \"carbohydrates\", \"carbohydrates\", \"cat\", \"cat\", \"cat\", \"cat\", \"cause\", \"cause\", \"cause\", \"cause\", \"childhood\", \"childhood\", \"childhood\", \"childhood\", \"children\", \"children\", \"children\", \"children\", \"cholesterol\", \"cholesterol\", \"cholesterol\", \"cholesterol\", \"chronic\", \"chronic\", \"chronic\", \"chronic\", \"cinderella\", \"click\", \"click\", \"click\", \"click\", \"com\", \"com\", \"com\", \"com\", \"command\", \"command\", \"command\", \"command\", \"composition\", \"composition\", \"composition\", \"composition\", \"condition\", \"condition\", \"condition\", \"condition\", \"consider\", \"consider\", \"consider\", \"consider\", \"contain\", \"contain\", \"contain\", \"contain\", \"cooper\", \"cooper\", \"cooper\", \"cooper\", \"cosmopolitan\", \"country\", \"country\", \"country\", \"country\", \"court\", \"court\", \"court\", \"court\", \"cov\", \"cov\", \"cov\", \"covid\", \"covid\", \"covid\", \"covid\", \"damini\", \"daycares\", \"daycares\", \"depression\", \"depression\", \"depression\", \"depression\", \"detail\", \"detail\", \"detail\", \"detail\", \"devdiscourse\", \"diabetes\", \"diabetes\", \"diabetes\", \"diabetes\", \"diabetwatch\", \"diet\", \"diet\", \"diet\", \"diet\", \"disease\", \"disease\", \"disease\", \"disease\", \"diseases\", \"diseases\", \"diseases\", \"diseases\", \"disorder\", \"disorder\", \"disorder\", \"disorder\", \"dog\", \"dog\", \"dog\", \"dog\", \"donald\", \"donald\", \"donald\", \"donald\", \"dosage\", \"dosage\", \"dosage\", \"drucker\", \"duo\", \"duo\", \"eat\", \"eat\", \"eat\", \"eat\", \"edo\", \"edo\", \"effect\", \"effect\", \"effect\", \"effect\", \"ekpakperan\", \"energy\", \"energy\", \"energy\", \"energy\", \"enhancers\", \"enhancers\", \"enhancers\", \"epidemic\", \"epidemic\", \"epidemic\", \"epidemic\", \"epilepsy\", \"epilepsy\", \"epilepsy\", \"epilepsy\", \"exercise\", \"exercise\", \"exercise\", \"exercise\", \"exhaustion\", \"exhaustion\", \"exhaustion\", \"exhaustion\", \"factor\", \"factor\", \"factor\", \"factor\", \"fat\", \"fat\", \"fat\", \"fat\", \"fatty\", \"fatty\", \"fatty\", \"fatty\", \"faty\", \"faty\", \"faty\", \"faty\", \"favour\", \"favour\", \"favour\", \"favour\", \"favourite\", \"favourite\", \"favourite\", \"favourite\", \"figure\", \"figure\", \"figure\", \"figure\", \"financial\", \"financial\", \"financial\", \"financial\", \"find\", \"find\", \"find\", \"find\", \"finland\", \"fit\", \"fit\", \"fit\", \"fit\", \"flail\", \"flail\", \"flail\", \"fluke\", \"food\", \"food\", \"food\", \"food\", \"foods\", \"foods\", \"foods\", \"foods\", \"forever\", \"forever\", \"forever\", \"forever\", \"formula\", \"formula\", \"formula\", \"formula\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"frugal\", \"frugal\", \"furnace\", \"gaikwad\", \"gaya\", \"get\", \"get\", \"get\", \"get\", \"glenn\", \"glenn\", \"glory\", \"glory\", \"glory\", \"glory\", \"hancock\", \"hancock\", \"health\", \"health\", \"health\", \"health\", \"healthy\", \"healthy\", \"healthy\", \"healthy\", \"heart\", \"heart\", \"heart\", \"heart\", \"helo\", \"help\", \"help\", \"help\", \"help\", \"herbalifechange\", \"high\", \"high\", \"high\", \"high\", \"higher\", \"higher\", \"higher\", \"higher\", \"highest\", \"highest\", \"highest\", \"highest\", \"hospital\", \"hospital\", \"hospital\", \"hospital\", \"hospitalize\", \"hospitalize\", \"hospitalize\", \"hospitalize\", \"hotline\", \"hotline\", \"hui\", \"hypertension\", \"hypertension\", \"hypertension\", \"hypertension\", \"ifa\", \"ifa\", \"ifaponmile\", \"ija\", \"ijebu\", \"ilalabas\", \"ilalabas\", \"immunocompromised\", \"immunocompromised\", \"immunocompromised\", \"immunocompromised\", \"impotence\", \"impotence\", \"impotence\", \"impotence\", \"inch\", \"inch\", \"inch\", \"inch\", \"include\", \"include\", \"include\", \"include\", \"increase\", \"increase\", \"increase\", \"increase\", \"inequities\", \"inflammation\", \"inflammation\", \"inflammation\", \"inflammation\", \"info\", \"info\", \"info\", \"info\", \"insomnia\", \"insomnia\", \"insomnia\", \"insomnia\", \"insulin\", \"insulin\", \"insulin\", \"insulin\", \"intensify\", \"intensive\", \"intensive\", \"intensive\", \"intensive\", \"irritableness\", \"isang\", \"isang\", \"isang\", \"jel\", \"jel\", \"johnson\", \"johnson\", \"johnson\", \"johnson\", \"kapag\", \"karti\", \"katawan\", \"kaya\", \"kaya\", \"kaya\", \"kaya\", \"kidney\", \"kidney\", \"kidney\", \"kidney\", \"kino\", \"klapers\", \"klapers\", \"klapers\", \"kondisyon\", \"konkol\", \"lahat\", \"lahat\", \"lahat\", \"lahat\", \"land\", \"land\", \"land\", \"land\", \"lead\", \"lead\", \"lead\", \"lead\", \"level\", \"level\", \"level\", \"level\", \"like\", \"like\", \"like\", \"like\", \"likely\", \"likely\", \"likely\", \"likely\", \"link\", \"link\", \"link\", \"link\", \"liquor\", \"liquor\", \"lishou\", \"lishou\", \"live\", \"live\", \"live\", \"live\", \"liver\", \"liver\", \"liver\", \"liver\", \"locsin\", \"loob\", \"lose\", \"lose\", \"lose\", \"lose\", \"loss\", \"loss\", \"loss\", \"loss\", \"lossing\", \"lotto\", \"lotto\", \"lotto\", \"lotto\", \"love\", \"love\", \"love\", \"love\", \"low\", \"low\", \"low\", \"low\", \"luck\", \"luck\", \"luck\", \"luck\", \"ly\", \"ly\", \"ly\", \"ly\", \"magkaroon\", \"mai\", \"mai\", \"maradona\", \"maradona\", \"marriage\", \"marriage\", \"marriage\", \"marriage\", \"mataas\", \"mataas\", \"mataas\", \"mataas\", \"may\", \"may\", \"may\", \"may\", \"mcg\", \"mcg\", \"mcg\", \"mcg\", \"meals\", \"meals\", \"meals\", \"meals\", \"metabolism\", \"metabolism\", \"metabolism\", \"metabolism\", \"million\", \"million\", \"million\", \"million\", \"module\", \"monarch\", \"monarch\", \"monarch\", \"month\", \"month\", \"month\", \"month\", \"morbidly\", \"morbidly\", \"morbidly\", \"morbidly\", \"mymi\", \"nagkakaroon\", \"naman\", \"nancy\", \"nancy\", \"nationwide\", \"nationwide\", \"nationwide\", \"nationwide\", \"naughton\", \"new\", \"new\", \"new\", \"new\", \"ng\", \"ng\", \"ng\", \"ng\", \"nicely\", \"obese\", \"obese\", \"obese\", \"obese\", \"obeseeither\", \"obesity\", \"obesity\", \"obesity\", \"obesity\", \"obesitycareweek\", \"ocw\", \"ogbu\", \"ohioans\", \"ohioans\", \"ohioans\", \"oildok\", \"okinawa\", \"okinawa\", \"omega\", \"omega\", \"omega\", \"omega\", \"one\", \"one\", \"one\", \"one\", \"onor\", \"overweight\", \"overweight\", \"overweight\", \"overweight\", \"pag\", \"pag\", \"pag\", \"pag\", \"pagtaba\", \"palainom\", \"pallavi\", \"pallavi\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pareshan\", \"patients\", \"patients\", \"patients\", \"patients\", \"pcod\", \"pcod\", \"pcod\", \"pcod\", \"pcos\", \"pcos\", \"pcos\", \"pcos\", \"pelosi\", \"pelosi\", \"pelosi\", \"people\", \"people\", \"people\", \"people\", \"percent\", \"percent\", \"percent\", \"percent\", \"pet\", \"pet\", \"pet\", \"pet\", \"pinterest\", \"plz\", \"plz\", \"plz\", \"plz\", \"political\", \"political\", \"political\", \"political\", \"post\", \"post\", \"post\", \"post\", \"pregnancy\", \"pregnancy\", \"pregnancy\", \"pregnancy\", \"president\", \"president\", \"president\", \"president\", \"pressure\", \"pressure\", \"pressure\", \"pressure\", \"prevent\", \"prevent\", \"prevent\", \"prevent\", \"problema\", \"problema\", \"problems\", \"problems\", \"problems\", \"problems\", \"process\", \"process\", \"process\", \"process\", \"program\", \"program\", \"program\", \"program\", \"promise\", \"promise\", \"promise\", \"promise\", \"promotion\", \"promotion\", \"promotion\", \"promotion\", \"protection\", \"protection\", \"protection\", \"protection\", \"proteect\", \"proteect\", \"protein\", \"protein\", \"protein\", \"protein\", \"protesters\", \"rand\", \"recover\", \"recover\", \"recover\", \"recover\", \"reduce\", \"reduce\", \"reduce\", \"reduce\", \"relieve\", \"relieve\", \"relieve\", \"relieve\", \"remove\", \"remove\", \"remove\", \"remove\", \"report\", \"report\", \"report\", \"report\", \"rewire\", \"ring\", \"ring\", \"ring\", \"ring\", \"risk\", \"risk\", \"risk\", \"risk\", \"ritual\", \"ritual\", \"ritual\", \"ritual\", \"rm\", \"rm\", \"robert\", \"robert\", \"robert\", \"robert\", \"rome\", \"rome\", \"sa\", \"sa\", \"sa\", \"sa\", \"sakit\", \"salt\", \"salt\", \"salt\", \"salt\", \"satiety\", \"satiety\", \"satiety\", \"satiety\", \"say\", \"say\", \"say\", \"say\", \"semaglutide\", \"semaglutide\", \"semaglutide\", \"severe\", \"severe\", \"severe\", \"severe\", \"severely\", \"severely\", \"severely\", \"severely\", \"side\", \"side\", \"side\", \"side\", \"skin\", \"skin\", \"skin\", \"skin\", \"skip\", \"skip\", \"skip\", \"skip\", \"sleep\", \"sleep\", \"sleep\", \"sleep\", \"slim\", \"slim\", \"slim\", \"slim\", \"soccer\", \"soccer\", \"soccer\", \"softgel\", \"softgel\", \"softgel\", \"softgels\", \"softgels\", \"speaker\", \"speaker\", \"speaker\", \"speaker\", \"spiritual\", \"spiritual\", \"spiritual\", \"spiritual\", \"state\", \"state\", \"state\", \"state\", \"stifle\", \"stifle\", \"stifle\", \"stifle\", \"stress\", \"stress\", \"stress\", \"stress\", \"stroke\", \"stroke\", \"stroke\", \"stroke\", \"study\", \"study\", \"study\", \"study\", \"success\", \"success\", \"success\", \"success\", \"sugar\", \"sugar\", \"sugar\", \"sugar\", \"superfoods\", \"superfoods\", \"survey\", \"survey\", \"survey\", \"survey\", \"symptomps\", \"third\", \"third\", \"third\", \"third\", \"thirds\", \"thirds\", \"thirds\", \"thirds\", \"thyroid\", \"thyroid\", \"thyroid\", \"thyroid\", \"time\", \"time\", \"time\", \"time\", \"tinatawag\", \"tinatawag\", \"trans\", \"trans\", \"trans\", \"trans\", \"trump\", \"trump\", \"trump\", \"trump\", \"tummy\", \"tummy\", \"tummy\", \"tummy\", \"tunde\", \"tunde\", \"turtle\", \"turtle\", \"tush\", \"type\", \"type\", \"type\", \"type\", \"uk\", \"uk\", \"uk\", \"uk\", \"uncontrollable\", \"uncontrollable\", \"unite\", \"unite\", \"unite\", \"unite\", \"ur\", \"ur\", \"ur\", \"ur\", \"urinatary\", \"urinatary\", \"urinatary\", \"url\", \"url\", \"url\", \"url\", \"urlstruggling\", \"us\", \"us\", \"us\", \"us\", \"veterinarians\", \"veterinarians\", \"veterinarians\", \"veterinarians\", \"viii\", \"viii\", \"viii\", \"visa\", \"visa\", \"visa\", \"visa\", \"vishwakarma\", \"vlckd\", \"vlckd\", \"vlckd\", \"webinar\", \"webinar\", \"webinar\", \"weight\", \"weight\", \"weight\", \"weight\", \"whatsapp\", \"whatsapp\", \"whatsapp\", \"whatsapp\", \"whtasp\", \"whtasp\", \"whtasp\", \"without\", \"without\", \"without\", \"without\", \"wmc\", \"wmc\", \"wmc\", \"wmc\", \"womb\", \"womb\", \"womb\", \"womb\", \"wood\", \"wood\", \"wood\", \"wood\", \"world\", \"world\", \"world\", \"world\", \"worldobesity\", \"www\", \"www\", \"www\", \"www\", \"xxl\", \"xxl\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el44071405931475259367043550536\", ldavis_el44071405931475259367043550536_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el44071405931475259367043550536\", ldavis_el44071405931475259367043550536_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el44071405931475259367043550536\", ldavis_el44071405931475259367043550536_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = pyLDAvis.gensim_models.prepare(topic_model = lda_model, \n",
    "                       corpus = corpus, \n",
    "                       dictionary = id2word)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_probabilities = []\n",
    "[topic_probabilities.append(i) for i in lda_model[corpus]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "topic_probs_dict = {i: topic_probabilities[i] for i in range(0, len(topic_probabilities))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame.from_dict({k:dict(v) for k,v in topic_probs_dict.items()}, orient='index').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "test.sort_values('index').to_csv(\"220108_lda_topic_probabilities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to Reduce Size for Silhouette Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(dat_obesity_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many dimensions for 99% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9900737710376136"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_[0:46]) #First 46: 99% of the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=46)\n",
    "pca_matrix = pca.fit_transform(dat_obesity_fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Sample 10% to Identify Best Cluster with Silhouette Score (Kmeans Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18721125561528673"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(pca_matrix, \n",
    "                 yhat_kmeans_centroids, \n",
    "                 sample_size = 6299,\n",
    "                 random_state = 110295) #10% of data, silhouette score = 0.225\n",
    "\n",
    "silhouette_score(pca_matrix, \n",
    "                 yhat_kmeans_plus, \n",
    "                 sample_size = 6299,\n",
    "                 random_state = 110295) #10% of data, silhouette score = 0.187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silhouette Score Using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Label based on LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "test = test.sort_values('index')\n",
    "test = test.fillna(0)\n",
    "test.columns = [\"index\", \"topic_1\", \"topic_2\", \"topic_3\", \"topic_4\"]\n",
    "test[\"lda_topic\"] = test.loc[:, \"topic_1\":\"topic_4\"].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.01761640141585322"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(pca_matrix, \n",
    "                 test.lda_topic, \n",
    "                 sample_size = 6299,\n",
    "                 random_state = 110295) #10% of data, silhouette score = -0.02"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffe67d4efdf50a67adb541926f1b56bfeeb48b191d1b9a6489e883c5acad5bda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
