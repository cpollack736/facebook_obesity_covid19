{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T14:16:34.377487Z",
     "start_time": "2021-10-28T14:16:33.957656Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    "import preprocessor as p \n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replacement(text, token_type = \"url\"):\n",
    "    \"\"\"\n",
    "    Function that will take in a block of text and replace the url with a token of some type\n",
    "    text (str): A block of text that contains a url\n",
    "    token_type (str): A specfication on what token should replace the url. Default is \"url\", which just returns \"url\". Other options include: \n",
    "        \"domain\", which returns the domain (e.g., \".gov url\")\n",
    "        \"host\", which returns the host of the website and domain (e.g., \"cdc.gov url\")\n",
    "    \"\"\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # Get a list of all urls\n",
    "    \n",
    "    if urls == []: # If list is blank\n",
    "        return text #Return previous text since nothing needs to change\n",
    "    \n",
    "    new_text = text[:] #Deep copy of the text to a new object\n",
    "    \n",
    "    if token_type == \"url\": #For the base case\n",
    "        for url in urls:\n",
    "            new_text = new_text.replace(url, \"url\") #Replace each url with the \"url\" token\n",
    "\n",
    "    elif token_type == \"domain\": #For the case of just extracting the domain\n",
    "        for url in urls:\n",
    "            try: \n",
    "                urlparse(url).netloc.split(\".\")[-1] #Extract just the domain \n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc.split(\".\")[-1] #Extract just the domain\n",
    "                \n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "            \n",
    "    else: #Extracting full host name\n",
    "        for url in urls:\n",
    "            try:\n",
    "                urlparse(url).netloc #Extract the full host name\n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc #Extract the full host name\n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_score(sid, text, dataframe):\n",
    "    \"\"\"\n",
    "    Function that will take in a text and return an estimated valence. \n",
    "    Note that this assumes that the column names match the VADER output\n",
    "    (i.e., \"neg\", \"neu\", \"pos\", \"compound\")\n",
    "    param sid (str): Name of the SentimentIntensityAnalyzer() defined outside the function \n",
    "    param text (str): A string of text to analyze\n",
    "    param dataframe (DataFrame): The pandas dataframe to append results to\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return(dataframe.append(scores, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T14:45:43.092298Z",
     "start_time": "2021-10-28T14:45:40.930504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (2,7,8,9,10,11,15,18,19,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"/Users/catherinepollack/Documents/dartmouth/research/aim3_facebook_covid19_obesity/data/\"\n",
    "#facebook_1 = pd.read_csv(str(data_file_path + \"210526_facebook_obesity_1.csv\"))\n",
    "#facebook_2 = pd.read_csv(str(data_file_path + \"210526_facebook_obesity_2.csv\"))\n",
    "#facebook = pd.concat([facebook_1, facebook_2], axis = 0)\n",
    "#facebook = facebook.reset_index()\n",
    "#facebook.to_csv(data_file_path + \"220119_combined_facebook_data.csv\")\n",
    "\n",
    "instagram_liwc = pd.read_csv(data_file_path + \"220128_obesity_instagram_liwc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname_mapping = dict(zip(list(instagram_liwc.columns[0:22]), list(instagram_liwc.loc[0, \"A\":\"V\"])))\n",
    "instagram_liwc = instagram_liwc.rename(columns = colname_mapping)\n",
    "instagram_liwc = instagram_liwc.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Followers at Posting</th>\n",
       "      <th>Post Created</th>\n",
       "      <th>Post Created Date</th>\n",
       "      <th>Post Created Time</th>\n",
       "      <th>Type</th>\n",
       "      <th>Total Interactions</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Madelaine Petsch</td>\n",
       "      <td>madelame</td>\n",
       "      <td>22034634</td>\n",
       "      <td>2020-07-29 11:36:38 EDT</td>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>11:36:38</td>\n",
       "      <td>Photo</td>\n",
       "      <td>1 624 548</td>\n",
       "      <td>1624521</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madelaine Petsch</td>\n",
       "      <td>madelame</td>\n",
       "      <td>22034634</td>\n",
       "      <td>2020-08-06 11:12:27 EDT</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>11:12:27</td>\n",
       "      <td>Photo</td>\n",
       "      <td>1 478 909</td>\n",
       "      <td>1478828</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIG BOSS üîëüîë</td>\n",
       "      <td>keke</td>\n",
       "      <td>10285242</td>\n",
       "      <td>2020-12-01 15:29:45 EST</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>15:29:45</td>\n",
       "      <td>Album</td>\n",
       "      <td>1 351 318</td>\n",
       "      <td>1282835</td>\n",
       "      <td>68483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelle - Weightloss‚¨áÔ∏è145LBS</td>\n",
       "      <td>michobabyy</td>\n",
       "      <td>433211</td>\n",
       "      <td>2020-11-03 10:18:16 EST</td>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>10:18:16</td>\n",
       "      <td>Video</td>\n",
       "      <td>1 086 092</td>\n",
       "      <td>1058004</td>\n",
       "      <td>28088</td>\n",
       "      <td>...</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lizzo</td>\n",
       "      <td>lizzobeeating</td>\n",
       "      <td>9851742</td>\n",
       "      <td>2021-03-06 17:30:35 EST</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>17:30:35</td>\n",
       "      <td>Video</td>\n",
       "      <td>643 410</td>\n",
       "      <td>620898</td>\n",
       "      <td>22512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Account      User Name Followers at Posting  \\\n",
       "1               Madelaine Petsch       madelame             22034634   \n",
       "2               Madelaine Petsch       madelame             22034634   \n",
       "3                    BIG BOSS üîëüîë           keke             10285242   \n",
       "4  Michelle - Weightloss‚¨áÔ∏è145LBS     michobabyy               433211   \n",
       "5                          Lizzo  lizzobeeating              9851742   \n",
       "\n",
       "              Post Created Post Created Date Post Created Time   Type  \\\n",
       "1  2020-07-29 11:36:38 EDT        2020-07-29          11:36:38  Photo   \n",
       "2  2020-08-06 11:12:27 EDT        2020-08-06          11:12:27  Photo   \n",
       "3  2020-12-01 15:29:45 EST        2020-12-01          15:29:45  Album   \n",
       "4  2020-11-03 10:18:16 EST        2020-11-03          10:18:16  Video   \n",
       "5  2021-03-06 17:30:35 EST        2021-03-06          17:30:35  Video   \n",
       "\n",
       "  Total Interactions    Likes Comments  ... Comma Colon SemiC QMark Exclam  \\\n",
       "1          1 624 548  1624521       27  ...  6.85   0.0   0.0  0.40   0.00   \n",
       "2          1 478 909  1478828       81  ...  6.30   0.0   0.0  0.42   0.42   \n",
       "3          1 351 318  1282835    68483  ...  1.91   0.0   0.0  0.00   0.95   \n",
       "4          1 086 092  1058004    28088  ...  5.88   0.0   0.0  0.00   0.27   \n",
       "5            643 410   620898    22512  ...  0.00   0.0   0.0  0.00   0.00   \n",
       "\n",
       "   Dash Quote Apostro Parenth OtherP  \n",
       "1  0.81  0.00    1.61    0.81   5.65  \n",
       "2  0.42  0.00    1.26    0.84   5.88  \n",
       "3  0.00  1.43    2.86    0.00   0.00  \n",
       "4  1.07  0.00    1.60    0.00   3.74  \n",
       "5  0.00  0.00    5.88    0.00   0.00  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagram_liwc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing text!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now processing text!\")\n",
    "instagram_liwc['processed_text'] = instagram_liwc.Description.apply(str) #Change to string\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "instagram_liwc['processed_text_bert'] = instagram_liwc['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(str.lower) #Convert to lowercase\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tokenizing\n",
      "Now removing stop words\n",
      "Now lemmatizing\n",
      "Now converting back to final string\n"
     ]
    }
   ],
   "source": [
    "print(\"Now tokenizing\")\n",
    "instagram_liwc['tokens'] = instagram_liwc.processed_text.apply(word_tokenize)\n",
    "\n",
    "print(\"Now removing stop words\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "instagram_liwc['tokens'] = instagram_liwc.tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "print(\"Now lemmatizing\")\n",
    "lemma = WordNetLemmatizer()\n",
    "instagram_liwc['tokens'] = instagram_liwc.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])\n",
    "\n",
    "print(\"Now converting back to final string\")\n",
    "\n",
    "instagram_liwc['final_text'] = instagram_liwc.tokens.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matrix Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Rankings\n",
      "             term         rank\n",
      "72    weight loss  5607.240581\n",
      "41       link bio  4956.064225\n",
      "44    lose weight  3411.313634\n",
      "33  heart disease  2504.785511\n",
      "10        com url  2082.859644\n",
      "['blood pressure', 'blood sugar', 'body fat', 'body mass', 'body weight', 'cardiovascular disease', 'change life', 'childhood obesity', 'chronic diseases', 'click link', 'com url', 'de peso', 'diabetes heart', 'diabetes obesity', 'diet exercise', 'disease diabetes', 'disease obesity', 'eat healthy', 'et al', 'every day', 'fat loss', 'feel like', 'fruit vegetables', 'gain weight', 'health benefit', 'health condition', 'health fitness', 'health issue', 'health problems', 'healthy diet', 'healthy lifestyle', 'healthy weight', 'heart attack', 'heart disease', 'help reduce', 'high blood', 'high cholesterol', 'immune system', 'increase risk', 'insulin resistance', 'join us', 'link bio', 'long term', 'look like', 'lose weight', 'lower risk', 'ly url', 'make sure', 'many people', 'meal plan', 'medical condition', 'mental health', 'morbidly obese', 'obesity diabetes', 'obesity heart', 'obesity type', 'overweight obese', 'per day', 'physical activity', 'plant base', 'process foods', 'reduce risk', 'risk factor', 'risk heart', 'risk obesity', 'side effect', 'social media', 'study show', 'take care', 'type diabetes', 'want lose', 'weight gain', 'weight loss', 'years ago', 'years old']\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(instagram_liwc['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(instagram_liwc['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running sentiment analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in instagram_liwc['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            instagram_liwc.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/220131_feature_matrix_no_bert_instagram.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(data_file_path + file_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffe67d4efdf50a67adb541926f1b56bfeeb48b191d1b9a6489e883c5acad5bda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
