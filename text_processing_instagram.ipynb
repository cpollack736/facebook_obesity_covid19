{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T14:16:34.377487Z",
     "start_time": "2021-10-28T14:16:33.957656Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    "import preprocessor as p \n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replacement(text, token_type = \"url\"):\n",
    "    \"\"\"\n",
    "    Function that will take in a block of text and replace the url with a token of some type\n",
    "    text (str): A block of text that contains a url\n",
    "    token_type (str): A specfication on what token should replace the url. Default is \"url\", which just returns \"url\". Other options include: \n",
    "        \"domain\", which returns the domain (e.g., \".gov url\")\n",
    "        \"host\", which returns the host of the website and domain (e.g., \"cdc.gov url\")\n",
    "    \"\"\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # Get a list of all urls\n",
    "    \n",
    "    if urls == []: # If list is blank\n",
    "        return text #Return previous text since nothing needs to change\n",
    "    \n",
    "    new_text = text[:] #Deep copy of the text to a new object\n",
    "    \n",
    "    if token_type == \"url\": #For the base case\n",
    "        for url in urls:\n",
    "            new_text = new_text.replace(url, \"url\") #Replace each url with the \"url\" token\n",
    "\n",
    "    elif token_type == \"domain\": #For the case of just extracting the domain\n",
    "        for url in urls:\n",
    "            try: \n",
    "                urlparse(url).netloc.split(\".\")[-1] #Extract just the domain \n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc.split(\".\")[-1] #Extract just the domain\n",
    "                \n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "            \n",
    "    else: #Extracting full host name\n",
    "        for url in urls:\n",
    "            try:\n",
    "                urlparse(url).netloc #Extract the full host name\n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc #Extract the full host name\n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_score(sid, text, dataframe):\n",
    "    \"\"\"\n",
    "    Function that will take in a text and return an estimated valence. \n",
    "    Note that this assumes that the column names match the VADER output\n",
    "    (i.e., \"neg\", \"neu\", \"pos\", \"compound\")\n",
    "    param sid (str): Name of the SentimentIntensityAnalyzer() defined outside the function \n",
    "    param text (str): A string of text to analyze\n",
    "    param dataframe (DataFrame): The pandas dataframe to append results to\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return(dataframe.append(scores, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T14:45:43.092298Z",
     "start_time": "2021-10-28T14:45:40.930504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (2,7,8,9,10,11,15,18,19,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"/Users/catherinepollack/Documents/dartmouth/research/aim3_facebook_covid19_obesity/data/\"\n",
    "#facebook_1 = pd.read_csv(str(data_file_path + \"210526_facebook_obesity_1.csv\"))\n",
    "#facebook_2 = pd.read_csv(str(data_file_path + \"210526_facebook_obesity_2.csv\"))\n",
    "#facebook = pd.concat([facebook_1, facebook_2], axis = 0)\n",
    "#facebook = facebook.reset_index()\n",
    "#facebook.to_csv(data_file_path + \"220119_combined_facebook_data.csv\")\n",
    "\n",
    "instagram_liwc = pd.read_csv(data_file_path + \"220128_obesity_instagram_liwc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname_mapping = dict(zip(list(instagram_liwc.columns[0:22]), list(instagram_liwc.loc[0, \"A\":\"V\"])))\n",
    "instagram_liwc = instagram_liwc.rename(columns = colname_mapping)\n",
    "instagram_liwc = instagram_liwc.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Followers at Posting</th>\n",
       "      <th>Post Created</th>\n",
       "      <th>Post Created Date</th>\n",
       "      <th>Post Created Time</th>\n",
       "      <th>Type</th>\n",
       "      <th>Total Interactions</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Madelaine Petsch</td>\n",
       "      <td>madelame</td>\n",
       "      <td>22034634</td>\n",
       "      <td>2020-07-29 11:36:38 EDT</td>\n",
       "      <td>2020-07-29</td>\n",
       "      <td>11:36:38</td>\n",
       "      <td>Photo</td>\n",
       "      <td>1 624 548</td>\n",
       "      <td>1624521</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.81</td>\n",
       "      <td>5.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Madelaine Petsch</td>\n",
       "      <td>madelame</td>\n",
       "      <td>22034634</td>\n",
       "      <td>2020-08-06 11:12:27 EDT</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>11:12:27</td>\n",
       "      <td>Photo</td>\n",
       "      <td>1 478 909</td>\n",
       "      <td>1478828</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BIG BOSS 🔑🔑</td>\n",
       "      <td>keke</td>\n",
       "      <td>10285242</td>\n",
       "      <td>2020-12-01 15:29:45 EST</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>15:29:45</td>\n",
       "      <td>Album</td>\n",
       "      <td>1 351 318</td>\n",
       "      <td>1282835</td>\n",
       "      <td>68483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Michelle - Weightloss⬇️145LBS</td>\n",
       "      <td>michobabyy</td>\n",
       "      <td>433211</td>\n",
       "      <td>2020-11-03 10:18:16 EST</td>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>10:18:16</td>\n",
       "      <td>Video</td>\n",
       "      <td>1 086 092</td>\n",
       "      <td>1058004</td>\n",
       "      <td>28088</td>\n",
       "      <td>...</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lizzo</td>\n",
       "      <td>lizzobeeating</td>\n",
       "      <td>9851742</td>\n",
       "      <td>2021-03-06 17:30:35 EST</td>\n",
       "      <td>2021-03-06</td>\n",
       "      <td>17:30:35</td>\n",
       "      <td>Video</td>\n",
       "      <td>643 410</td>\n",
       "      <td>620898</td>\n",
       "      <td>22512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Account      User Name Followers at Posting  \\\n",
       "1               Madelaine Petsch       madelame             22034634   \n",
       "2               Madelaine Petsch       madelame             22034634   \n",
       "3                    BIG BOSS 🔑🔑           keke             10285242   \n",
       "4  Michelle - Weightloss⬇️145LBS     michobabyy               433211   \n",
       "5                          Lizzo  lizzobeeating              9851742   \n",
       "\n",
       "              Post Created Post Created Date Post Created Time   Type  \\\n",
       "1  2020-07-29 11:36:38 EDT        2020-07-29          11:36:38  Photo   \n",
       "2  2020-08-06 11:12:27 EDT        2020-08-06          11:12:27  Photo   \n",
       "3  2020-12-01 15:29:45 EST        2020-12-01          15:29:45  Album   \n",
       "4  2020-11-03 10:18:16 EST        2020-11-03          10:18:16  Video   \n",
       "5  2021-03-06 17:30:35 EST        2021-03-06          17:30:35  Video   \n",
       "\n",
       "  Total Interactions    Likes Comments  ... Comma Colon SemiC QMark Exclam  \\\n",
       "1          1 624 548  1624521       27  ...  6.85   0.0   0.0  0.40   0.00   \n",
       "2          1 478 909  1478828       81  ...  6.30   0.0   0.0  0.42   0.42   \n",
       "3          1 351 318  1282835    68483  ...  1.91   0.0   0.0  0.00   0.95   \n",
       "4          1 086 092  1058004    28088  ...  5.88   0.0   0.0  0.00   0.27   \n",
       "5            643 410   620898    22512  ...  0.00   0.0   0.0  0.00   0.00   \n",
       "\n",
       "   Dash Quote Apostro Parenth OtherP  \n",
       "1  0.81  0.00    1.61    0.81   5.65  \n",
       "2  0.42  0.00    1.26    0.84   5.88  \n",
       "3  0.00  1.43    2.86    0.00   0.00  \n",
       "4  1.07  0.00    1.60    0.00   3.74  \n",
       "5  0.00  0.00    5.88    0.00   0.00  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagram_liwc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing text!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now processing text!\")\n",
    "instagram_liwc['processed_text'] = instagram_liwc.Description.apply(str) #Change to string\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "instagram_liwc['processed_text_bert'] = instagram_liwc['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(str.lower) #Convert to lowercase\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tokenizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-2b409486851a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  facebook_liwc_no_pet['tokens'] = facebook_liwc_no_pet.processed_text.apply(word_tokenize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now removing stop words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-2b409486851a>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  facebook_liwc_no_pet['tokens'] = facebook_liwc_no_pet.tokens.apply(lambda x: [item for item in x if item not in stop_words])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lemmatizing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-2b409486851a>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  facebook_liwc_no_pet['tokens'] = facebook_liwc_no_pet.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now converting back to final string\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-2b409486851a>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  facebook_liwc_no_pet['final_text'] = facebook_liwc_no_pet.tokens.apply(lambda x: ' '.join(x))\n"
     ]
    }
   ],
   "source": [
    "print(\"Now tokenizing\")\n",
    "instagram_liwc['tokens'] = instagram_liwc.processed_text.apply(word_tokenize)\n",
    "\n",
    "print(\"Now removing stop words\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "instagram_liwc['tokens'] = instagram_liwc.tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "print(\"Now lemmatizing\")\n",
    "lemma = WordNetLemmatizer()\n",
    "instagram_liwc['tokens'] = instagram_liwc.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])\n",
    "\n",
    "print(\"Now converting back to final string\")\n",
    "\n",
    "instagram_liwc['final_text'] = instagram_liwc.tokens.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matrix Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Rankings\n",
      "              term          rank\n",
      "44         com url  37808.973986\n",
      "336    weight loss  26146.941193\n",
      "193         ly url  15374.250588\n",
      "186    lose weight  14369.514303\n",
      "138  heart disease  13899.112503\n",
      "['abnormal hormone', 'abnormal level', 'abuse poor', 'acquire secondary', 'alcohol consumption', 'alcohol drug', 'also help', 'anxiety alcohol', 'anxiety issue', 'api whatsapp', 'arm reduce', 'atherosclerosis gloom', 'attack land', 'back fruit', 'bad dream', 'baldness ulcers', 'become slim', 'begin wth', 'belly fat', 'best result', 'best without', 'bite ly', 'blood pressure', 'blood sugar', 'body fat', 'body image', 'body mass', 'body weight', 'brain chemicals', 'business boom', 'call neurotransmitters', 'call whatsapp', 'cancer baldness', 'capsule natural', 'cardiovascular disease', 'cause cause', 'cause early', 'challenge reduce', 'chemicals call', 'chemicals low', 'childhood obesity', 'chronic diseases', 'click link', 'climax occur', 'com url', 'command tone', 'confidence relationships', 'consumption solutuon', 'contact us', 'control intimacy', 'court case', 'damage atherosclerosis', 'days challenge', 'decrease confidence', 'decrease libido', 'depression abnormal', 'depression prostate', 'depression worry', 'desire early', 'detail inbox', 'detail whatsapp', 'develop previous', 'diabetes diabetic', 'diabetes heart', 'diabetes high', 'diabetes multiple', 'diabetes obesity', 'diabetic obesity', 'diet exercise', 'diet without', 'disease diabetes', 'disease obesity', 'disease thyroid', 'dream promise', 'drug diabetes', 'drug use', 'early ejaculation', 'early sexual', 'effect skip', 'effect without', 'effective permanent', 'ejaculate sooner', 'ejaculation cause', 'ejaculation guilty', 'ejaculation occur', 'ejaculatory problems', 'erectile dysfunction', 'erection decrease', 'esteem smoke', 'every day', 'examination success', 'excessive alcohol', 'excitment much', 'exercise side', 'exercise without', 'exhaustion improve', 'experience sexual', 'experience without', 'facebook com', 'family history', 'fat obesity', 'fat post', 'fat thyroid', 'fatigue exhaustion', 'fatigue obesity', 'favour ring', 'fear uncertainties', 'fedup mummy', 'feel increase', 'fight obesity', 'figure reduce', 'figure thighs', 'financial breakthrough', 'first sexual', 'fit figure', 'fit remove', 'forever detail', 'formula relieve', 'fruit womb', 'get back', 'get days', 'get rid', 'get slim', 'gloom fear', 'good luck', 'guarantee weight', 'guide best', 'guilty feel', 'heal male', 'health benefit', 'health care', 'health condition', 'health issue', 'health problems', 'health product', 'healthy lifestyle', 'healthy weight', 'heart attack', 'heart disease', 'help call', 'help get', 'help prevent', 'help reduce', 'high blood', 'high cholesterol', 'high risk', 'higher risk', 'hormonal imbalance', 'hormone imbalance', 'hormone level', 'hypertension diabetes', 'hypertension prostate', 'image depression', 'imbalance abnormal', 'imbalance multiple', 'immune system', 'impotence permanently', 'improve vitality', 'inability maintain', 'inch loss', 'increase risk', 'increase ur', 'infection prostate', 'inflammation infection', 'inherit traits', 'intensive formula', 'intercourse acquire', 'intercourse partner', 'intercourse sexual', 'intimacy decrease', 'issue relate', 'join us', 'join weight', 'joint pain', 'kgs want', 'kidney disease', 'land court', 'level brain', 'level hormonal', 'libido sexual', 'life long', 'like orgasm', 'liver disease', 'long primary', 'long term', 'lose glory', 'lose weight', 'loss best', 'loss inch', 'loss program', 'love ring', 'low self', 'lower risk', 'ly url', 'maintain erection', 'maintain healthy', 'make sure', 'male impotence', 'man ejaculate', 'management course', 'many people', 'marriage success', 'mass index', 'masturbation depression', 'may also', 'meals get', 'meals slim', 'medical condition', 'mental health', 'morbidly obese', 'much stimulation', 'multiple sclerosis', 'mummy tummy', 'narcotics hormone', 'natural health', 'nearly time', 'nerve damage', 'neurotransmitters inflammation', 'new study', 'obesity diabetes', 'obesity diet', 'obesity fedup', 'obesity pcod', 'obesity problems', 'obesity stroke', 'occur man', 'occur nearly', 'occur sooner', 'org url', 'orgasm climax', 'overweight obese', 'ow ly', 'partner would', 'pcod pcos', 'pcos obesity', 'per day', 'permanent treatment', 'permanently help', 'physical activity', 'political appointment', 'poor body', 'post pregnancy', 'pregnancy fat', 'premature ejaculation', 'pressure hypertension', 'prevent heal', 'previous sexual', 'primary occur', 'problems drug', 'problems relationships', 'problems stress', 'product usa', 'program get', 'promise fail', 'promotion work', 'prostate cancer', 'prostate disease', 'prostate surgery', 'prostate urethra', 'protection ring', 'public health', 'recover lose', 'reduce arm', 'reduce get', 'reduce kgs', 'reduce risk', 'reduce weight', 'relate control', 'relationship stress', 'relationships depression', 'relationships inability', 'relieve fatigue', 'remove belly', 'result join', 'result weight', 'ring favour', 'ring marriage', 'ring recover', 'risk develop', 'risk factor', 'risk obesity', 'rush sexual', 'sadness fatigue', 'sclerosis nerve', 'sclerosis prostate', 'secondary develop', 'self esteem', 'sexual abuse', 'sexual desire', 'sexual experience', 'sexual intercourse', 'sexual unexperience', 'sickle cell', 'side effect', 'skip meals', 'slim body', 'slim fit', 'slim forever', 'smoke narcotics', 'solutuon xpowerman', 'sooner sexual', 'sooner want', 'spend get', 'spiritual attack', 'stimulation relationship', 'stress anxiety', 'study show', 'success love', 'sugar level', 'surgery masturbation', 'tendency rush', 'thighs reduce', 'thyroid obesity', 'thyroid problems', 'time begin', 'tone protection', 'traits diabetes', 'treatment intensive', 'tummy guide', 'type diabetes', 'ulcers use', 'uncertainties sadness', 'unexperience excitment', 'unite state', 'ur first', 'ur tendency', 'urethra inherit', 'usa effective', 'use chemicals', 'use excessive', 'visa approval', 'vitality prevent', 'wa url', 'want become', 'want life', 'weight gain', 'weight loss', 'weight management', 'whatsapp call', 'whatsapp com', 'without diet', 'without ejaculatory', 'without exercise', 'without side', 'without skip', 'work command', 'worry premature', 'would like', 'wth ur', 'www facebook', 'xpowerman capsule', 'year old', 'years old']\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(facebook_liwc_no_pet['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(facebook_liwc_no_pet['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running sentiment analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in facebook_liwc_no_pet['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            facebook_liwc_no_pet.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/220120_feature_matrix_no_bert.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(data_file_path + file_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffe67d4efdf50a67adb541926f1b56bfeeb48b191d1b9a6489e883c5acad5bda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
