{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T14:16:34.377487Z",
     "start_time": "2021-10-28T14:16:33.957656Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    "import preprocessor as p \n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_replacement(text, token_type = \"url\"):\n",
    "    \"\"\"\n",
    "    Function that will take in a block of text and replace the url with a token of some type\n",
    "    text (str): A block of text that contains a url\n",
    "    token_type (str): A specfication on what token should replace the url. Default is \"url\", which just returns \"url\". Other options include: \n",
    "        \"domain\", which returns the domain (e.g., \".gov url\")\n",
    "        \"host\", which returns the host of the website and domain (e.g., \"cdc.gov url\")\n",
    "    \"\"\"\n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    # Get a list of all urls\n",
    "    \n",
    "    if urls == []: # If list is blank\n",
    "        return text #Return previous text since nothing needs to change\n",
    "    \n",
    "    new_text = text[:] #Deep copy of the text to a new object\n",
    "    \n",
    "    if token_type == \"url\": #For the base case\n",
    "        for url in urls:\n",
    "            new_text = new_text.replace(url, \"url\") #Replace each url with the \"url\" token\n",
    "\n",
    "    elif token_type == \"domain\": #For the case of just extracting the domain\n",
    "        for url in urls:\n",
    "            try: \n",
    "                urlparse(url).netloc.split(\".\")[-1] #Extract just the domain \n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc.split(\".\")[-1] #Extract just the domain\n",
    "                \n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "            \n",
    "    else: #Extracting full host name\n",
    "        for url in urls:\n",
    "            try:\n",
    "                urlparse(url).netloc #Extract the full host name\n",
    "            except ValueError:\n",
    "                print(str(\"URL parse error with \" + url))\n",
    "                domain = \"\"\n",
    "            else:\n",
    "                domain = urlparse(url).netloc #Extract the full host name\n",
    "            domain_url = domain + \" url\"\n",
    "            new_text = new_text.replace(url, domain_url)\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vader_score(sid, text, dataframe):\n",
    "    \"\"\"\n",
    "    Function that will take in a text and return an estimated valence. \n",
    "    Note that this assumes that the column names match the VADER output\n",
    "    (i.e., \"neg\", \"neu\", \"pos\", \"compound\")\n",
    "    param sid (str): Name of the SentimentIntensityAnalyzer() defined outside the function \n",
    "    param text (str): A string of text to analyze\n",
    "    param dataframe (DataFrame): The pandas dataframe to append results to\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return(dataframe.append(scores, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T14:45:43.092298Z",
     "start_time": "2021-10-28T14:45:40.930504Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (2,7,8,9,10,11,15,18,19,20,21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data_file_path = \"/Users/catherinepollack/Documents/dartmouth/research/aim3_facebook_covid19_obesity/data/\"\n",
    "#facebook_1 = pd.read_csv(str(data_file_path + \"210526_facebook_obesity_1.csv\"))\n",
    "#facebook_2 = pd.read_csv(str(data_file_path + \"210526_facebook_obesity_2.csv\"))\n",
    "#facebook = pd.concat([facebook_1, facebook_2], axis = 0)\n",
    "#facebook = facebook.reset_index()\n",
    "#facebook.to_csv(data_file_path + \"220119_combined_facebook_data.csv\")\n",
    "\n",
    "instagram_liwc = pd.read_csv(data_file_path + \"220204_non_health_control_instagram_liwc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Columns with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colname_mapping = dict(zip(list(instagram_liwc.columns[0:22]), list(instagram_liwc.loc[0, \"A\":\"V\"])))\n",
    "instagram_liwc = instagram_liwc.rename(columns = colname_mapping)\n",
    "instagram_liwc = instagram_liwc.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>User Name</th>\n",
       "      <th>Followers at Posting</th>\n",
       "      <th>Post Created</th>\n",
       "      <th>Post Created Date</th>\n",
       "      <th>Post Created Time</th>\n",
       "      <th>Type</th>\n",
       "      <th>Total Interactions</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9GAG: Go Fun The World</td>\n",
       "      <td>9gag</td>\n",
       "      <td>50345255</td>\n",
       "      <td>2019-04-04 23:01:12 EDT</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>23:01:12</td>\n",
       "      <td>Video</td>\n",
       "      <td>622 442</td>\n",
       "      <td>572695</td>\n",
       "      <td>49747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instagram</td>\n",
       "      <td>instagram</td>\n",
       "      <td>273642637</td>\n",
       "      <td>2019-01-04 14:39:30 EST</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>14:39:30</td>\n",
       "      <td>Photo</td>\n",
       "      <td>607 980</td>\n",
       "      <td>600071</td>\n",
       "      <td>7909</td>\n",
       "      <td>...</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.84</td>\n",
       "      <td>4.91</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jack antonoff</td>\n",
       "      <td>jackantonoff</td>\n",
       "      <td>371822</td>\n",
       "      <td>2020-07-24 10:34:12 EDT</td>\n",
       "      <td>2020-07-24</td>\n",
       "      <td>10:34:12</td>\n",
       "      <td>Photo</td>\n",
       "      <td>126 257</td>\n",
       "      <td>123140</td>\n",
       "      <td>3117</td>\n",
       "      <td>...</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Overheard New York</td>\n",
       "      <td>overheardnewyork</td>\n",
       "      <td>1472824</td>\n",
       "      <td>2021-03-15 12:13:22 EDT</td>\n",
       "      <td>2021-03-15</td>\n",
       "      <td>12:13:22</td>\n",
       "      <td>Photo</td>\n",
       "      <td>104 944</td>\n",
       "      <td>103912</td>\n",
       "      <td>1032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rini (Harini Raghavan)</td>\n",
       "      <td>rinimusic</td>\n",
       "      <td>18878</td>\n",
       "      <td>2020-03-26 21:57:39 EDT</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>21:57:39</td>\n",
       "      <td>Video</td>\n",
       "      <td>94 918</td>\n",
       "      <td>94683</td>\n",
       "      <td>235</td>\n",
       "      <td>...</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.94</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Account         User Name Followers at Posting  \\\n",
       "1  9GAG: Go Fun The World              9gag             50345255   \n",
       "2               Instagram         instagram            273642637   \n",
       "3           jack antonoff      jackantonoff               371822   \n",
       "4      Overheard New York  overheardnewyork              1472824   \n",
       "5  Rini (Harini Raghavan)         rinimusic                18878   \n",
       "\n",
       "              Post Created Post Created Date Post Created Time   Type  \\\n",
       "1  2019-04-04 23:01:12 EDT        2019-04-04          23:01:12  Video   \n",
       "2  2019-01-04 14:39:30 EST        2019-01-04          14:39:30  Photo   \n",
       "3  2020-07-24 10:34:12 EDT        2020-07-24          10:34:12  Photo   \n",
       "4  2021-03-15 12:13:22 EDT        2021-03-15          12:13:22  Photo   \n",
       "5  2020-03-26 21:57:39 EDT        2020-03-26          21:57:39  Video   \n",
       "\n",
       "  Total Interactions   Likes Comments  ... Comma Colon SemiC QMark Exclam  \\\n",
       "1            622 442  572695    49747  ...  0.00  0.00   0.0   0.0   0.00   \n",
       "2            607 980  600071     7909  ...  6.75  0.61   0.0   0.0   0.00   \n",
       "3            126 257  123140     3117  ...  1.79  0.60   0.0   0.0   0.00   \n",
       "4            104 944  103912     1032  ...  0.00  0.00   0.0   0.0   0.00   \n",
       "5             94 918   94683      235  ...  1.23  0.00   0.0   0.0   4.94   \n",
       "\n",
       "    Dash Quote Apostro Parenth OtherP  \n",
       "1  20.00  0.00    0.00    0.00  50.00  \n",
       "2   1.84  4.91    3.68    1.23   1.23  \n",
       "3   0.30  0.00    1.19    0.00   2.39  \n",
       "4   0.00  0.00    0.00    0.00  33.33  \n",
       "5   1.23  1.23    1.23    0.00  13.58  \n",
       "\n",
       "[5 rows x 115 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagram_liwc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now processing text!\n",
      "URL parse error with https://taipeimaf.com]\n"
     ]
    }
   ],
   "source": [
    "print(\"Now processing text!\")\n",
    "instagram_liwc['processed_text'] = instagram_liwc.Description.apply(str) #Change to string\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(html.unescape) #Remove HTML escape characters\n",
    "instagram_liwc['processed_text_bert'] = instagram_liwc['processed_text'] #Create new column for BERT-specific embeddings (don't want to remove additional information)\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: url_replacement(x, \"host\"))\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(p.clean) #Preprocessor removes hashtags and cleans text\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(str.lower) #Convert to lowercase\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: ''.join([i for i in x if not i.isdigit()])) #Remove numbers\n",
    "instagram_liwc['processed_text'] = instagram_liwc.processed_text.apply(lambda x: re.sub('[^a-zA-z]', \" \", x)) #Remove non-letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now tokenizing\n",
      "Now removing stop words\n",
      "Now lemmatizing\n",
      "Now converting back to final string\n"
     ]
    }
   ],
   "source": [
    "print(\"Now tokenizing\")\n",
    "instagram_liwc['tokens'] = instagram_liwc.processed_text.apply(word_tokenize)\n",
    "\n",
    "print(\"Now removing stop words\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "instagram_liwc['tokens'] = instagram_liwc.tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "print(\"Now lemmatizing\")\n",
    "lemma = WordNetLemmatizer()\n",
    "instagram_liwc['tokens'] = instagram_liwc.tokens.apply(lambda x: [lemma.lemmatize(word = w, pos = 'v') for w in x])\n",
    "\n",
    "print(\"Now converting back to final string\")\n",
    "\n",
    "instagram_liwc['final_text'] = instagram_liwc.tokens.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matrix Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running TFIDF!\n",
      "Count Vectorizer\n",
      "Get Feature Names\n",
      "TFIDF Vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/catherinepollack/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Rankings\n",
      "                  term         rank\n",
      "16            link bio  1589.639510\n",
      "0        bass clarinet   686.058867\n",
      "23       play clarinet   583.488713\n",
      "8         follow daily   437.870319\n",
      "26  principal clarinet   297.771456\n",
      "['bass clarinet', 'bio listen', 'chamber music', 'chat soon', 'clarinet concerto', 'classical music', 'click link', 'com url', 'follow daily', 'full version', 'full video', 'happy birthday', 'happy practice', 'high school', 'hit follow', 'join us', 'link bio', 'listen full', 'look forward', 'much love', 'new york', 'niv week', 'part video', 'play clarinet', 'playlist bio', 'please hit', 'principal clarinet', 'version post', 'week playlist', 'youtube channel']\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running TFIDF!\")\n",
    "count_vectorizer = CountVectorizer(ngram_range = (2,2), min_df = 0.01, max_df = 0.75)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df = 0.01, max_df = 0.75)\n",
    "print(\"Count Vectorizer\")\n",
    "count_matrix = count_vectorizer.fit_transform(instagram_liwc['final_text'])\n",
    "print(\"Get Feature Names\")\n",
    "features = count_vectorizer.get_feature_names()\n",
    "print(\"TFIDF Vectorizer\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(instagram_liwc['final_text'])\n",
    "scores = np.asarray(tfidf_matrix)\n",
    "sums = tfidf_matrix.sum(axis = 0)\n",
    "data = []\n",
    "print(\"Compiling Rankings\")\n",
    "for col, term in enumerate(features):\n",
    "    data.append((term, sums[0,col]))\n",
    "ranking = pd.DataFrame(data, columns = ['term', 'rank'])\n",
    "words = ranking.sort_values('rank', ascending = False)\n",
    "print(words.head())\n",
    "\n",
    "tf_idf_dataframe = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "# Creating list of terms that can be used as column names\n",
    "terms = []\n",
    "for col, term in enumerate(features):\n",
    "    terms.append(term)\n",
    "print(terms)\n",
    "\n",
    "tf_idf_dataframe.columns = [x for x in terms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running sentiment analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running sentiment analysis!\")\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiments = pd.DataFrame(columns = ['neg', 'neu', 'pos', 'compound'])\n",
    "for text in instagram_liwc['processed_text_bert']:\n",
    "    sentiments = get_vader_score(sid, text, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now running feature matrix!\n"
     ]
    }
   ],
   "source": [
    "print(\"Now running feature matrix!\")\n",
    "feature_matrix_no_bert = pd.concat([tf_idf_dataframe.reset_index(), \n",
    "                            sentiments.reset_index(), \n",
    "                            instagram_liwc.reset_index()],\n",
    "                          axis = 1)\n",
    "file_name = \"/220204_feature_matrix_no_bert_instagram_non_health_comparator.csv\"\n",
    "feature_matrix_no_bert.to_csv(str(data_file_path + file_name))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ffe67d4efdf50a67adb541926f1b56bfeeb48b191d1b9a6489e883c5acad5bda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
